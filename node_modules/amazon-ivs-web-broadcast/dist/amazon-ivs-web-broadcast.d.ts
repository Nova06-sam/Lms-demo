/**
 * Amazon Interactive Video Service (Amazon IVS) web broadcast SDK gives you the
 * ability to capture live video from web browsers and send as an input to an
 * Amazon IVS channel or stage. You can include it on new and existing
 * websites, with support for both desktop and mobile web browsers.
 *
 * @remarks
 *
 * The SDK enables one to broadcast to a low-latency IVS channel (HLS) and/or
 * broadcast and playback to a real-time IVS stage (WebRTC).
 *
 * To get started, see the relevant getting started guides for
 * [low-latency](https://docs.aws.amazon.com/ivs/latest/LowLatencyUserGuide/broadcast-web.html)
 * and
 * [real-time](https://docs.aws.amazon.com/ivs/latest/RealTimeUserGuide/broadcast-web.html)
 * streaming.
 *
 * @packageDocumentation
 */

export declare const __version: string;

declare enum Action {
    PUBLISH = "publish",
    SUBSCRIBE = "subscribe",
    JOIN = "join",
    LEAVE = "leave"
}

declare const ADD_DEVICE_COMPOSITION_INDEX_MISSING_ERROR: {
    readonly name: "AddDeviceCompositionIndexMissingError";
    readonly code: 4003;
    readonly message: "VideoComposition's \"index\" property is missing";
};

declare const ADD_DEVICE_COMPOSITION_MISSING_ERROR: {
    readonly name: "AddDeviceCompositionMissingError";
    readonly code: 4002;
    readonly message: "VideoComposition is missing";
};

declare const ADD_DEVICE_CONSTRAINTS_ERROR: {
    readonly name: "AddDeviceConstraintsError";
    readonly code: 4006;
    readonly message: "Constraints error";
};

declare const ADD_DEVICE_DEVICE_ERROR: {
    readonly name: "AddDeviceError";
    readonly code: 4000;
    readonly message: "Device is missing";
};

declare const ADD_DEVICE_NAME_EXISTS_ERROR: {
    readonly name: "AddDeviceNameExistsError";
    readonly code: 4001;
    readonly message: "Name is already registered";
};

declare const ADD_DEVICE_NAME_MISSING_ERROR: {
    readonly name: "AddDeviceNameMissingError";
    readonly code: 4004;
    readonly message: "Name property is missing";
};

declare const ADD_DEVICE_UNSUPPORTED: {
    name: string;
    code: number;
    message: string;
};

/**
 * A class implementing the Web Broadcast SDK API.
 */
export declare class AmazonIVSBroadcastClient {
    #private;
    config: Config;
    emitter: IEventEmitter;
    private url?;
    private analyticsTracker;
    private isValid;
    private peerClient;
    private peerStatsTracker;
    private broadcastCanvasManager;
    private mediaStreamManager;
    /**
     * Creates an instance of the AmazonIVSBroadcastClient.
     *
     * @param config - {@link BroadcastClientConfig}
     */
    constructor(config: Config);
    /**
     * Explicitly stop and/or free internal components that would otherwise leak.
     */
    delete(): void;
    /**
     * Starts broadcasting to IVS.
     *
     * @throws Throws a {@link BroadcastClientError} if unable to stream.
     * @param streamKey - IVS-generated stream key for your IVS channel.
     * @param ingestEndpoint - IVS-generated ingest endpoint for your IVS channel. This is optional but will need to be
     * specified in either the config or as a parameter of this function.
     */
    startBroadcast(streamKey: string, ingestEndpoint?: string): Promise<BroadcastClientError | void>;
    /**
     * Stops the broadcast.
     */
    stopBroadcast(): void;
    /**
     * Gets the dimensions of the canvas on which the stream is drawn.
     */
    getCanvasDimensions(): CanvasDimensions;
    /**
     * Adds the specified video input to the broadcast stream.
     *
     * @param device - Video input to add to the stream.
     * @param name - Unique string you assign to the video-input device.
     * @param position - The desired position of the video in the stream.
     */
    addVideoInputDevice(device: MediaStream, name: string, position: VideoComposition): Promise<void>;
    /**
     * Adds the specified image to the broadcast stream.
     *
     * @param image - Image to add to the stream. Must be of type HTMLImageElement, HTMLVideoElement,  HTMLCanvasElement or ImageBitmap.
     * @param name - Unique string you assign to the image.
     * @param position - The desired position of the image in the stream.
     */
    addImageSource(image: HTMLImageElement | HTMLVideoElement | HTMLCanvasElement | ImageBitmap, name: string, position: VideoComposition): Promise<void>;
    /**
     * Adds the specified audio input to the broadcast stream.
     *
     * @param device - Audio input to add to the stream.
     * @param name - Unique string you assign to the audio-input device.
     */
    addAudioInputDevice(device: MediaStream, name: string): Promise<void>;
    /**
     * Removes the specified video-input device from the stream.
     *
     * @param name - unique name of the device to remove.
     */
    removeVideoInputDevice(name: string): void;
    /**
     * Removes the specified audio-input device from the stream.
     *
     * @param name - Unique name of the device to remove.
     */
    removeAudioInputDevice(name: string): void;
    /**
     * Removes the specified image from the stream.
     *
     * @param name - Unique name of the image to remove.
     */
    removeImage(name: string): void;
    /**
     * Updates the position of the specified video device in the stream.
     *
     * @param name - Unique name of the device to update.
     * @param composition - The new position.
     */
    updateVideoDeviceComposition(name: string, composition: VideoComposition): void;
    /**
     * Exchanges (swaps) the VideoComposition settings of two video devices.
     *
     * @param name1 - Unique name of the video device to swap (one of two).
     * @param name2 - Unique name of the video device to swap (two of two).
     */
    exchangeVideoDevicePositions(name1: string, name2: string): void;
    /**
     * Gets the MediaStream of the specified audio device.
     *
     * @param name - Unique name of the audio device.
     */
    getAudioInputDevice(name: string): ReturnType<BroadcastCanvasManager['getAudioInputDevice']>;
    /**
     * Gets the {@link VideoDevice} of the specified video-input device.
     *
     * @param name - Unique name of the video-input device.
     */
    getVideoInputDevice(name: string): ReturnType<BroadcastCanvasManager['getVideoInputDevice']>;
    /**
     * Gets the state of the webRTC connection.
     */
    getConnectionState(): ConnectionState;
    /**
     * Get active session ID
     */
    getSessionId(): string | undefined;
    on<Key extends string>(event: Key, callback: Callback): void;
    off<Key extends string>(event: Key, callback: Callback): void;
    /**
     * Returns the Media Stream Manager's Audio Context to allow a user to have more direct control over the audio stream.
     */
    getAudioContext(): AudioContext;
    /**
     * Attach a HTMLCanvasElement to display a preview of the output.
     *
     * @param element - An HTMLCanvasElement on which to draw the composited preview.
     */
    attachPreview(element: HTMLCanvasElement): void;
    /**
     * Detach the HTMLCanvasElement preview element.
     *
     */
    detachPreview(): void;
    /**
     * Disables all video content from the stream.
     */
    disableVideo(): void;
    /**
     * Enables all video content from the stream if previously disabled.
     */
    enableVideo(): void;
    /**
     * Disables all audio content from the stream.
     */
    disableAudio(): void;
    /**
     * Enables all audio content from the stream if previously disabled.
     */
    enableAudio(): void;
}

declare interface AnalyticsEvent {
    data: BerryessaEvent;
    transformProperties(...args: unknown[]): BerryessaEvent['properties'];
}

declare enum AnalyticsEvents {
    PUBLISHED_VIDEO_STATS = "ivs_broadcast_webrtc_published_video_stats",
    PUBLISHED_VIDEO_STATS_WINDOW = "ivs_broadcast_webrtc_published_video_stats_window",
    PUBLISHED_AUDIO_STATS = "ivs_broadcast_webrtc_published_audio_stats",
    PUBLISHED_AUDIO_STATS_WINDOW = "ivs_broadcast_webrtc_published_audio_stats_window",
    SUBSCRIBED_VIDEO_STATS = "ivs_broadcast_webrtc_subscribed_video_stats",
    SUBSCRIBED_VIDEO_STATS_WINDOW = "ivs_broadcast_webrtc_sbcrbd_video_stats_window",
    SUBSCRIBED_AUDIO_STATS = "ivs_broadcast_webrtc_subscribed_audio_stats",
    SUBSCRIBED_AUDIO_STATS_WINDOW = "ivs_broadcast_webrtc_sbcrbd_audio_stats_window",
    PUBLISH = "ivs_broadcast_multihost_publish",
    UNPUBLISH = "ivs_broadcast_multihost_unpublish",
    SUBSCRIBE = "ivs_broadcast_multihost_subscribe",
    UNSUBSCRIBE = "ivs_broadcast_multihost_unsubscribe",
    PUBLISH_STARTED = "ivs_broadcast_multihost_publish_started",
    PUBLISH_FAILED = "ivs_broadcast_multihost_publish_failed",
    PUBLISH_ENDED = "ivs_broadcast_multihost_publish_ended",
    STATE_UPDATED = "ivs_broadcast_multihost_event_state_updated",
    SUBSCRIBE_STARTED = "ivs_broadcast_multihost_subscribe_started",
    SUBSCRIBE_FAILED = "ivs_broadcast_multihost_subscribe_failed",
    SUBSCRIBE_ENDED = "ivs_broadcast_multihost_subscribe_ended",
    FIRST_FRAME = "ivs_broadcast_multihost_first_frame",
    JOIN = "ivs_broadcast_multihost_join",
    JOIN_ATTEMPT = "ivs_broadcast_multihost_event_connect_attempt",
    JOIN_ATTEMPT_FAILED = "ivs_broadcast_multihost_event_connect_failed",
    JOIN_ENDED = "ivs_broadcast_multihost_join_ended",
    LEAVE = "ivs_broadcast_multihost_leave",
    REASSIGNMENT_REQUEST = "ivs_broadcast_multihost_reassignment_request",
    SERVER_REQUEST = "ivs_broadcast_multihost_server_request",
    TRACE = "ivs_broadcast_stage_trace",
    ERROR = "ivs_broadcast_error",
    MINUTE = "ivs_broadcast_multihost_minute",
    CONNECTION_STATE = "ivs_broadcast_webrtc_connection_state",
    ICE_GATHERING_STATE = "ivs_broadcast_webrtc_gathering_state",
    EDP_CONNECTED = "ivs_broadcast_multihost_event_connected",
    EDP_DISCONNECTED = "ivs_broadcast_multihost_event_disconnected",
    EDP_PONG = "ivs_broadcast_multihost_edp_rtt",
    SIMULCAST_LAYER_INFO = "ivs_broadcast_simulcast_layer_info",
    MULTIHOST_CONFIGURATION = "ivs_broadcast_multihost_configuration",
    MULTIHOST_SUBSCRIBE_CONFIGURATION = "ivs_broadcast_multihost_subscribe_configuration",
    PLAYBACK_LAYER_REQUEST = "ivs_broadcast_multihost_playback_layer_request",
    PLAYBACK_LAYER_STATE = "ivs_broadcast_multihost_playback_layer_state",
    SELECTED_PAIR_CHANGE = "ivs_broadcast_webrtc_selected_pair_change",
    ANALYTICS_HEALTH_REPORT = "ivs_broadcast_analytics_health_report",
    DEVCONF_ERROR = "ivs_devconf_error",
    DEVCONF_OPS_METRICS = "ivs_devconf_ops_metrics",
    DEVCONF_TRACE = "ivs_devconf_trace",
    DEVCONF_VALUE = "ivs_devconf_value"
}

/**
 * Utility class that exposes methods to track events.
 */
declare class AnalyticsTracker {
    private sessionId;
    private sharedProperties;
    private eventSender;
    private flushInterval?;
    private queuedEvents;
    private errorReports;
    /**
     * Creates the AnalyticsTracker.
     *
     * @param postUrl - URL to emit events to.
     * @returns {AnalyticsTracker} A singleton instance of AnalyticsTracker
     */
    constructor();
    /**
     * Creates a new client-side session ID to track behavior.
     */
    start(): void;
    /**
     * Stop client and flush any pending events.
     *
     * @param isOnBeforeUnload - indicates if this stop is due to a beforeunload event
     */
    stop(isOnBeforeUnload: boolean): void;
    /**
     * Sends tracking data with shared properties.
     *
     * @param event - A BerryessaEvent or AnalyticsEvent
     * @param disableSharedProps - Send data without a majorit of the shared properties
     * @private
     */
    trackEvent(event: AnalyticsEvent | BerryessaEvent, disableSharedProps?: boolean): void;
    /**
     * Sends tracking data without a majority of the shared properties, just
     * session_id.
     *
     * @param event - A BerryessaEvent or AnalyticsEvent
     * @private
     */
    trackEventNoSharedProps(event: AnalyticsEvent | BerryessaEvent): void;
    /**
     * Sends tracking error to analytics endpoint and throws the error
     *
     * @param error - A StageClientError to track and throw.
     */
    trackErrorAndThrow(error: StageClientError): never;
    /**
     * Sends tracking error to analytics endpoint.
     *
     * @param err - A StageClientError to track.
     */
    trackError(err: StageClientError): void;
    /**
     * Periodically we also want to flush the error event map to ensure that any error events
     * outside of the window are flushed and removed from the map.
     *
     * We can optionally forcefully flush the map when the tracker is being cleaned up
     *
     * @param params - Configuration object to flush the error event map.
     * @param params.force - Forcefully flush the error event map
     */
    queueAggregateErrors({ force }: {
        force?: boolean | undefined;
    }): void;
    /**
     * Reported errors with a count of 0 have already been proactively
     * sent, so we'll skip sending those, but still delete them from the map
     *
     * @param report - ErrorReport to track
     */
    private trackErrorReportIfNeeded;
    /**
     * Sends tracking data to analytics endpint.
     *
     * @param analyticsEvents - A analytics event array
     * @private
     */
    batchEvents(analyticsEvents: AnalyticsEvent[]): void;
    /**
     * Initializes the AnalyticsTracker by setting the initial properties.
     *
     * @param parameters - Configuration object to initialize the AnalyticsTracker.
     * @param parameters.postUrl - URL to emit events to.
     * @private
     */
    private initSharedProperties;
    /**
     * Creates a new session ID and ensures its on shared properties
     */
    private genSessionId;
    /**
     * Flush any queued events as a single batch.
     *
     * @param useBeacon - flush events with Beacon API
     *
     * @private
     */
    private flushEvents;
    /**
     * Returns the session id
     *
     * @returns the session id
     */
    getSessionId(): string;
}

/**
 * A static `landscape` stream configuration for a `Basic` 1080p IVS account.
 *
 * @category Stream Config
 */
export declare const BASIC_FULL_HD_LANDSCAPE: StreamConfig;

/**
 * A static `portrait` stream configuration for a `Basic` 1080p IVS account.
 *
 * @category Stream Config
 */
export declare const BASIC_FULL_HD_PORTRAIT: StreamConfig;

/**
 * A static `landscape` stream configuration for a `Basic` IVS account.
 *
 * @category Stream Config
 */
export declare const BASIC_LANDSCAPE: StreamConfig;

/**
 * A static `portrait` stream configuration for a `Basic` IVS account.
 *
 * @category Stream Config
 */
export declare const BASIC_PORTRAIT: StreamConfig;

declare interface BerryessaEvent {
    event: EventNames;
    properties: Record<string, unknown>;
    critical?: boolean;
}

declare const BROADCAST_CONFIGURATION_ERROR: {
    readonly name: "BroadcastConfigurationError";
    readonly code: 11000;
    readonly message: "Error when configuring broadcast client";
};

declare const BROADCAST_ERROR: {
    readonly name: "BroadcastError";
    readonly code: 1000;
    readonly message: "Unable to broadcast";
};

/**
 * This class is responsible for ensuring that we have a stable audio and video stream to broadcast over the WebRTC
 * PeerConnection. For ingesting into IVS we don't want to have audio and video tracks being added or removed mid-stream.
 * Adding and removing tracks is very common in WebRTC and that standard way to handle things like muting or
 * changing video inputs. So the BroadcastCanvasManager acts as the adapter between these two worlds. Having this adapter
 * also gives us the ability support some different video operations, like compositing in the browser.
 *
 * The basic functionality is to take the input stream (audio or video) and pass it through some browser API that is
 * able to produce a new MediaStream. For audio this means we pass the microphone input to an AudioContext and for video
 * this means we pass the video input to a Canvas element. (For video you can't directly pass a MediaStreamTrack to a
 * canvas element. Instead we pass the MediaStreamTrack to a video element which can decode the video, and then we copy
 * the frame from the video element to the canvas element.) By passing the input streams through an intermediary browser
 * API in this way we introduce a seam where we are able to stop or remove an input track and the browser will continue
 * sending information from the browser API that is silent/blank.
 */
declare class BroadcastCanvasManager {
    private config;
    private mediaStreamManager;
    private analyticsTracker;
    audioContext: AudioContext;
    compositeStream: MediaStream;
    private compositeEl;
    private compositeContext;
    private schedulerWorker;
    private audioDestination;
    private running;
    private mixer;
    private previewElement;
    private previewCtx;
    private nextMix;
    /**
     * Instantiates the {@link BroadcastCanvasManager}
     *
     * @param config - {@link Config}
     */
    constructor(config: Config, mediaStreamManager: MediaStreamManager, analyticsTracker: AnalyticsTracker);
    /**
     * Attach a HTMLCanvasElement to display a preview of the output.
     *
     * @param element - An HTMLCanvasElement on which to draw the composited preview.
     */
    attachPreview(element: HTMLCanvasElement): void;
    /**
     * Detach the attached preview element.
     *
     */
    detachPreview(): void;
    /**
     * It creates a silent audio source that is connected to the audio context's
     * destination node.
     */
    private setupSilence;
    /**
     * If the audio context is suspended, then resume it and clean up the event listeners.
     */
    private unlockAudioContext;
    /**
     * Returns a tuple containing the available video track and audio track.
     */
    getTracks(): [Maybe<MediaStreamTrack>, Maybe<MediaStreamTrack>];
    /**
     * Returns the video track currently attached to the canvas.
     */
    getCanvasVideoTrack(): Maybe<MediaStreamTrack>;
    /**
     * Looks through the mixer for an audio device associated with the provided name.
     *
     * @param name - Name of an audio input device.
     */
    getAudioInputDevice(name: string): Maybe<MediaStream>;
    /**
     * Looks through the mixer for a video device associated with the provided name.
     *
     * @param name - Name of a video input device.
     */
    getVideoInputDevice(name: string): Maybe<VideoDevice>;
    /**
     * Returns an object containing the broadcast canvas' height and width.
     */
    getCanvasDimensions(): CanvasDimensions;
    /**
     * Adds a provided video input to the mixer and composites it to the canvas stream.
     *
     * @param videoStream - A video stream of type {@link MediaStream} to add to the mixer.
     * @param name - A unique identifier to associate to the device.
     * @param position - The position to composite the device on the canvas.
     */
    addVideoInputDevice(videoStream: MediaStream, name: string, position: VideoComposition): Promise<void>;
    /**
     * Adds a provided image to the mixer and composites it to the canvas stream.
     *
     * @param image - An image to add to the mixer.
     * @param name - A unique identifier to associate to the device.
     * @param position - The position to composite the device on the canvas.
     */
    addImageSource(image: HTMLImageElement | HTMLVideoElement | HTMLCanvasElement | ImageBitmap, name: string, position: VideoComposition): Promise<void>;
    /**
     * Adds a provided audio input to the mixer and composites it to the canvas stream.
     *
     * @param audioStream - An audio stream to add to the mixer.
     * @param name - A unique identifier to associate to the device.
     */
    addAudioInputDevice(audioStream: MediaStream, name: string): Promise<void>;
    /**
     * Removes the specified image from the mixer and the composite stream.
     *
     * @param name - A unique identifier to associate to the device.
     */
    removeImage(name: string): void;
    /**
     * Removes the specified video device from the mixer and the composite stream.
     *
     * @param name - A unique identifier to associate to the device
     */
    removeVideoInputDevice(name: string): void;
    /**
     * Removes the specified audio device from the mixer and the composite stream.
     *
     * @param name - A unique identifier to associate to the device
     */
    removeAudioInputDevice(name: string): void;
    /**
     * Queries for the specified video device in the mixer and updates its composition properties.
     *
     * @param name - A reference to the video device to modify.
     * @param composition - A {@link VideoComposition} configuration with which to update the video device's composition in the mixer and canvas stream.
     */
    updateVideoDeviceComposition(name: string, composition: VideoComposition): void;
    /**
     * Takes 2 video devices and swaps their positions in the composite stream.
     *
     * @param name1 - Name of the first device.
     * @param name2 - Name of the second device.
     */
    exchangeVideoDevicePositions(name1: string, name2: string): void;
    /**
     * Starts the {@link BroadcastCanvasManager} and the {@link Scheduler} to begin compositing.
     */
    start(): void;
    /**
     * Stops the {@link BroadcastCanvasManager} and terminates the {@link Scheduler}'s WebWorker.
     */
    stop(): void;
    /**
     * Returns a boolean value representing the state of {@link BroadcastCanvasManager#running}
     */
    isCapturing(): boolean;
    /**
     * Returns the WebAudio Context's baseLatency value.
     */
    getAudioLatency(): number;
    /**
     * Returns the video track's latency value, if available.
     */
    getVideoLatency(): number;
    /**
     * Prevents rendering of any video.
     */
    disableVideo(): void;
    /**
     * Enables rendering of all video.
     */
    enableVideo(): void;
    /**
     * Prevents rendering of any audio.
     */
    disableAudio(): void;
    /**
     * Enables rendering of all audio.
     */
    enableAudio(): void;
    /**
     * Get ideal x, y, width, height to center the source within the target
     * bounding box on the canvas.
     *
     * @param source - {@link VideoDevice} or {@link ImageDevice} that contains a CanvasImageSource
     * @param canvasWidth - canvas width
     * @param canvasHeight - canvas height
     * @returns x, y, width, height draw region within the canvas bounds
     */
    private getDrawRegion;
    /**
     * Composites all available video tracks in the mixer to the canvas.
     *
     * @private
     */
    private drawComposite;
    /**
     * Requests a frame capture from the canvas if the browser supports it.
     *
     * @private
     */
    private requestCanvasCapture;
    /**
     * Draws a black screen to the canvas.
     *
     * @private
     */
    private drawBlackScreen;
    /**
     * Looks through the provided list for the provided device and removes it from the list.
     *
     * @param name - A unique identifier to associate to the device.
     * @param list - One of the mixer's device arrays.
     * @private
     */
    private removeDevice;
    /**
     * Handles all incoming Worker thread events.
     *
     * @param event - A {@link MessageEvent} incoming from a Worker thread.
     * @private
     */
    private handleWorkerEvent;
    /**
     * Indication of whether or not we are waiting on a new video frame to be generated for a given frame rate.
     *
     * This essentially gates frames being generated at an uneven pace (e.g. Frame 1 @ 40ms, Frame 2 @ 42 ms) and
     * allows us to drop frames
     *
     * @param now - The datetime of the requested frame
     */
    private shouldMix;
}

/**
 * The configuration used to initialize the BroadcastClient.
 */
export declare interface BroadcastClientConfig {
    /**
     * URL used to communicate with the WebRTC backend.
     * If provided an rtmps url, we will convert it to a usable https url.
     */
    ingestEndpoint?: string;
    /**
     * A stream configuration matching one's IVS account configuration.
     */
    streamConfig?: StreamConfig;
    logLevel?: LogLevels;
    logger?: Partial<Logger>;
    networkReconnectConfig?: NetworkReconnectConfig;
}

/**
 * Class describing errors encountered when using the AmazonIVSBroadcastClient.
 */
export declare class BroadcastClientError extends Error {
    code: number;
    name: string;
    details?: string;
    params?: {
        [key: string]: unknown;
    };
    cause?: Error;
    /**
     * Creates the new error.
     *
     * @param ErrorParams - Object containing the Error parameters.
     */
    constructor({ name, message, code, details, cause, params }: ErrorParams);
}

/**
 * Helper interface for creating TypeScript-capable Event Emitter implementations.
 * This maps the {@link BroadcastClientEvents} to their respective payload type.
 */
export declare interface BroadcastClientEventPayloads {
    /**
     * Maps the {@link BroadcastClientEvents.ERROR} type to {@link BroadcastClientError}.
     */
    [BroadcastClientEvents.ERROR]: BroadcastClientError;
    /**
     * Maps the {@link BroadcastClientEvents.CONNECTION_STATE_CHANGE} type to 'string'.
     */
    [BroadcastClientEvents.CONNECTION_STATE_CHANGE]: string;
    /**
     * Maps the {@link BroadcastClientEvents.ACTIVE_STATE_CHANGE} type to 'boolean'.
     */
    [BroadcastClientEvents.ACTIVE_STATE_CHANGE]: boolean;
}

/**
 * An enumeration describing the events emitted from the AmazonIVSBroadcastClient.
 */
export declare const BroadcastClientEvents: {
    /**
     * Indicates that the webRTC connection state has changed.
     *
     * @param payload - {@link ConnectionState}
     * @event connectionStateChange
     */
    readonly CONNECTION_STATE_CHANGE: "connectionStateChange";
    /**
     * Indicates that the client has encountered an error.
     *
     * @param payload - {@link BroadcastClientError}
     * @event clientError
     */
    readonly ERROR: "clientError";
    /**
     * Indicates that the broadcast start/stop state has changed.
     *
     * @param payload - `boolean`
     * @event activeStateChanged
     */
    readonly ACTIVE_STATE_CHANGE: "activeStateChange";
};

declare enum BroadcastSessionEvents {
    START_BROADCAST = "ivs_broadcast_start_broadcast",
    STOP_BROADCAST = "ivs_broadcast_stop_broadcast",
    MINUTE_BROADCAST = "ivs_broadcast_minute_broadcast",
    WEBRTC_AUDIO_STATS = "ivs_broadcast_webrtc_audio_stats",
    WEBRTC_VIDEO_STATS = "ivs_broadcast_webrtc_video_stats",
    WEBRTC_AUDIO_STATS_WINDOW = "ivs_broadcast_webrtc_audio_stats_window",
    WEBRTC_VIDEO_STATS_WINDOW = "ivs_broadcast_webrtc_video_stats_window",
    CONNECTION_ESTABLISHED = "ivs_broadcast_connection_established",
    SESSION_VIDEO_PROPERTIES = "ivs_broadcast_session_video_properties",
    SESSION_AUDIO_PROPERTIES = "ivs_broadcast_session_audio_properties",
    SESSION_VIDEO_ENCODER_CONFIGURED = "ivs_broadcast_session_video_encoder_configured",
    SESSION_AUDIO_ENCODER_CONFIGURED = "ivs_broadcast_session_audio_encoder_configured",
    SOURCE_AUDIO_LATENCY = "ivs_broadcast_source_audio_latency",
    SOURCE_VIDEO_LATENCY = "ivs_broadcast_source_video_latency",
    INPUT_DEVICE_ATTACHED = "ivs_broadcast_input_device_attached",
    INPUT_DEVICE_DETACHED = "ivs_broadcast_input_device_detached",
    CONGESTION_TIME = "ivs_broadcast_congestion_time",
    BUFFER_DURATION = "ivs_broadcast_buffer_duration",
    MEASURED_BITRATE = "ivs_broadcast_measured_bitrate",
    RECOMMENDED_BITRATE = "ivs_broadcast_recommended_bitrate",
    CONNECTION_RTT = "ivs_broadcast_connection_rtt",
    ERROR = "ivs_broadcast_error"
}

export declare type Callback = () => unknown;

declare type Callback_2 = (...args: any[]) => void;

declare const CAMERA_ERROR: {
    readonly name: "CameraError";
    readonly code: 2000;
    readonly message: "Camera could not be captured";
};

/**
 * An interface to describe the dimensions of the HTML Canvas used to composite the stream.
 */
export declare interface CanvasDimensions {
    /**
     * Height of the canvas.
     */
    height: number;
    /**
     * Width of the canvas.
     */
    width: number;
}

declare type Claims = {
    resource: string;
    jti: string;
    events_url: string;
    whip_url: string;
    user_id?: string;
    exp?: number;
    version: string;
    attributes?: ClaimsAttributes;
    capabilities: ClaimsCapabilities;
    topic: string;
};

declare type ClaimsAttributes = {
    [key: string]: string;
};

declare type ClaimsCapabilities = {
    allow_publish?: boolean;
    allow_subscribe?: boolean;
};

declare const CLIENT_INVALID_ERROR: {
    readonly name: "ClientInvalidError";
    readonly code: 16000;
    readonly message: "Client is no longer valid, possibly due to delete() invocation.";
};

/**
 * The Config class is a singleton that holds the configuration for the application
 */
export declare class Config {
    #private;
    LOG_LEVEL: typeof LogLevels;
    BASIC_LANDSCAPE: StreamConfig;
    BASIC_PORTRAIT: StreamConfig;
    BASIC_FULL_HD_LANDSCAPE: StreamConfig;
    BASIC_FULL_HD_PORTRAIT: StreamConfig;
    STANDARD_LANDSCAPE: StreamConfig;
    STANDARD_PORTRAIT: StreamConfig;
    constructor(ingestEndpoint?: string, logLevel?: LogLevels, streamConfig?: StreamConfig, logger?: Partial<Logger>, networkReconnectConfig?: NetworkReconnectConfig);
    /**
     * Get the ingest endpoint from the config file.
     *
     * @returns The ingest endpoint.
     */
    get ingestEndpoint(): string | undefined;
    /**
     * Set the ingest endpoint to the new value, and validates that the new value is valid.
     * If the rtmps ingest server is provided, this function attempts to converts it to a valid 'https' endpoint.
     * If the provided ingest endpoint is invalid, it throws an error.
     *
     * @param newIngestEndpoint - The new value of the ingestEndpoint property.
     */
    set ingestEndpoint(newIngestEndpoint: string | undefined);
    /**
     * Get the stream config.
     *
     * @returns The streamConfig property is a getter that returns the streamConfig property of the class.
     */
    get streamConfig(): StreamConfig;
    /**
     * Set the streamConfig property to the newStreamConfig parameter.
     *
     * The function is a setter, so it has a special name
     *
     * @param newStreamConfig - The new stream configuration.
     */
    set streamConfig(newStreamConfig: unknown);
    /**
     * Get the log level.
     *
     * @returns The log level.
     */
    get logLevel(): LogLevels | undefined;
    /**
     * Set the log level to the given value.
     *
     * @param newLogLevel - The new log level to set.
     */
    set logLevel(newLogLevel: unknown);
    /**
     * Get the logger property from the config.
     *
     * @returns The logger property is being returned.
     */
    get logger(): Partial<Logger> | undefined;
    /**
     * Set the logger property.
     *
     * @param newLogger - The new logger to set.
     */
    set logger(newLogger: Partial<Logger> | undefined);
    /**
     * Get the network reconnect configuration
     *
     * @returns The network reconnect config property is being returned.
     */
    get networkReconnectConfig(): NetworkReconnectConfig | undefined;
    /**
     * Set the network reconnect config property.
     *
     * @param newNetworkReconnectConfig - The new network reconnect config to set.
     */
    set networkReconnectConfig(newNetworkReconnectConfig: NetworkReconnectConfig | undefined);
}

export declare type ConfigurationInstance = Partial<BroadcastClientConfig> & DefaultConfig;

/**
 * An enumeration describing the connection state of the webRTC stream.
 */
export declare enum ConnectionState {
    /**
     * Indicates the webRTC connection is closed.
     */
    CLOSED = "closed",
    /**
     * Indicates that the webRTC connection is completed.
     */
    COMPLETED = "completed",
    /**
     * Indicates that the webRTC connection is connected.
     */
    CONNECTED = "connected",
    /**
     * Indicates that the webRTC connection is connecting.
     */
    CONNECTING = "connecting",
    /**
     * Indicates that the webRTC connection is disconnected.
     */
    DISCONNECTED = "disconnected",
    /**
     * Indicates that the webRTC connection has failed.
     */
    FAILED = "failed",
    /**
     * Indicates that the webRTC connection is idle.
     */
    IDLE = "idle",
    /**
     * Indicates that a new webRTC connection has been created but has not yet started connecting to the server.
     */
    NEW = "new",
    /**
     * Indicates that the webRTC connection has been deleted or does not yet exist.
     */
    NONE = "none"
}

export declare function create(broadcastConfig: BroadcastClientConfig): AmazonIVSBroadcastClient;

export declare interface DefaultConfig {
    LOG_LEVEL: typeof LogLevels;
    BASIC_LANDSCAPE: typeof BASIC_LANDSCAPE;
    BASIC_PORTRAIT: typeof BASIC_PORTRAIT;
    BASIC_FULL_HD_LANDSCAPE: typeof BASIC_FULL_HD_LANDSCAPE;
    BASIC_FULL_HD_PORTRAIT: typeof BASIC_FULL_HD_PORTRAIT;
    STANDARD_LANDSCAPE: typeof STANDARD_LANDSCAPE;
    STANDARD_PORTRAIT: typeof STANDARD_PORTRAIT;
}

/**
 * A typed EventEmitter with events defined by BroadcastClientEventPayloads.
 */
export declare interface Emitter<EventMap> {
    _events?: {
        [Key: string]: Callback | Callback[];
    };
    on<Key extends keyof EventMap>(event: Key, callback: (value: EventMap[Key]) => void, context?: object): void;
    on<Key extends string>(event: Key, callback: Callback, context?: object): void;
    off<Key extends keyof EventMap>(event: Key, callback: (value: EventMap[Key]) => void, context?: object): void;
    off<Key extends string>(event: Key, callback: Callback, context?: object): void;
    emit<Key extends keyof EventMap>(event: Key, params: EventMap[Key]): void;
    emit<Key extends string, Params>(event: Key, params?: Params): void;
    removeAllListeners(): void;
}

export declare type ErrorFn = (args: ErrorLogParams) => void;

export declare interface ErrorLogParams extends LogParams {
    err: Error | unknown;
}

declare interface ErrorParams {
    message: string;
    code: number;
    name: string;
    details?: string;
    params?: {
        [key: string]: unknown;
    };
    cause?: Error;
}

declare namespace Errors {
    export {
        BROADCAST_ERROR,
        CAMERA_ERROR,
        INPUT_ERROR,
        ADD_DEVICE_DEVICE_ERROR,
        ADD_DEVICE_NAME_EXISTS_ERROR,
        ADD_DEVICE_COMPOSITION_MISSING_ERROR,
        ADD_DEVICE_COMPOSITION_INDEX_MISSING_ERROR,
        ADD_DEVICE_NAME_MISSING_ERROR,
        ADD_DEVICE_UNSUPPORTED,
        ADD_DEVICE_CONSTRAINTS_ERROR,
        REMOVE_DEVICE_NOT_FOUND_ERROR,
        REMOVE_IMAGE_NOT_FOUND_ERROR,
        UPDATE_VIDEO_DEVICE_COMPOSITION_MISSING_ERROR,
        UPDATE_VIDEO_DEVICE_COMPOSITION_INDEX_MISSING_ERROR,
        UPDATE_VIDEO_DEVICE_COMPOSITION_NAME_NOT_FOUND_ERROR,
        EXCHANGE_POSITION_DEVICE_NOT_FOUND_ERROR,
        FAILED_TO_ADD_IMAGE_ERROR,
        PEER_SETUP_ERROR,
        PEER_CONNECTION_ERROR,
        INVALID_STREAM_KEY,
        BROADCAST_CONFIGURATION_ERROR,
        STREAM_CONFIGURATION_ERROR,
        INGEST_ENDPOINT_TYPE_ERROR,
        INGEST_ENDPOINT_URL_ERROR,
        LOG_LEVEL_TYPE_ERROR,
        LOGGER_TYPE_ERROR,
        CLIENT_INVALID_ERROR,
        NETWORK_RECONNECT_CONFIGURATION_ERROR,
        NETWORK_RECONNECT_CONFIGURATION_INVALID_TIMEOUT_ERROR,
        STREAM_KEY_INVALID_CHAR_ERROR,
        STAGE_TOKEN_TYPE_ERROR,
        STAGE_WHIP_OVERRIDE_ERROR
    }
}
export { Errors }

declare type EventMap = {
    [key: string]: Callback_2;
};

declare type EventNames = Exclude<ValuesOf<typeof BroadcastSessionEvents>, 'buffer_duration'> | AnalyticsEvents;

declare const EXCHANGE_POSITION_DEVICE_NOT_FOUND_ERROR: {
    readonly name: "ExchangePositionDeviceNotFoundError";
    readonly code: 8000;
    readonly message: "Device with that name is not found";
};

declare const FAILED_TO_ADD_IMAGE_ERROR: {
    readonly name: "FailedToAddImageError";
    readonly code: 9000;
    readonly message: "Failed to add image";
};

declare type GetStatsFunction = (track: MediaStreamTrack) => Promise<PeerClientStats | undefined>;

declare type IEventEmitter = Emitter<BroadcastClientEventPayloads>;

export declare interface InBandMessagingStageVideoConfiguration {
    /**
     * Whether to enable or disable in-band messages for a stage stream
     *
     * @default false
     */
    enabled: boolean;
}

export declare interface InBandMessagingSubscribeConfiguration {
    /**
     * Whether to enable or disable in-band messages as a subscriber
     *
     * @default false
     */
    enabled: boolean;
}

declare const INGEST_ENDPOINT_TYPE_ERROR: {
    readonly name: "IngestEndpointTypeError";
    readonly code: 13000;
    readonly message: "Ingest endpoint must be a string";
};

declare const INGEST_ENDPOINT_URL_ERROR: {
    readonly name: "IngestEndpointUrlError";
    readonly code: 13001;
    readonly message: "Ingest endpoint must be a valid https or rtmps URL";
};

/**
 * Configuration property which dictates what layers are initially delivered to
 * Subscribers when simulcast is sent by the Publisher.
 *
 * Defaults to {@link InitialLayerPreference.LOWEST_QUALITY}
 *
 * @public
 */
export declare enum InitialLayerPreference {
    /**
     * The server will deliver the lowest quality layer of video first
     * optimizing bandwidth consumption, as well as time to media.
     *
     * Note that quality is defined as the combination of size, bitrate,
     * and framerate of the video. e.g. 1080p video is greater quality than
     * 720p video.
     */
    LOWEST_QUALITY = "lowest_quality",
    /**
     * The server will deliver the highest quality layer of video first
     * which optimizes quality, but may increase the time to media.
     *
     * Note that quality is defined as the combination of size, bitrate,
     * and framerate of the video. e.g. 1080p video is greater quality than
     * 720p video.
     */
    HIGHEST_QUALITY = "highest_quality"
}

declare const INPUT_ERROR: {
    readonly name: "InputError";
    readonly code: 3000;
    readonly message: "Input could not be attached";
};

declare const INVALID_STREAM_KEY: {
    readonly name: "InvalidStreamKey";
    readonly code: 10003;
    readonly message: "Invalid Stream Key.";
};

/**
 * Check for feature support in current environment.
 */
export declare const isSupported: () => boolean;

declare namespace IVSBroadcastClient {
    export {
        create,
        isSupported,
        __version,
        BroadcastClientEvents,
        ConnectionState,
        LogLevels,
        LogLevels as LOG_LEVEL,
        BASIC_LANDSCAPE,
        BASIC_PORTRAIT,
        BASIC_FULL_HD_LANDSCAPE,
        BASIC_FULL_HD_PORTRAIT,
        STANDARD_LANDSCAPE,
        STANDARD_PORTRAIT,
        Errors,
        BroadcastClientError,
        StreamConfig,
        NetworkReconnectConfig,
        ResolutionConfig,
        ConfigurationInstance,
        DefaultConfig,
        Callback,
        Emitter,
        BroadcastClientEventPayloads,
        CanvasDimensions,
        VideoComposition,
        VideoDevice,
        Logger,
        LoggerParams,
        LogParams,
        ErrorLogParams,
        ErrorFn,
        LogFn,
        Config,
        AmazonIVSBroadcastClient,
        BroadcastClientConfig,
        TraceRequest,
        SubscribeRequest,
        SubscribeResponse,
        PublishResponse,
        WHIPServerResponse,
        WHIPResource,
        SfuResource,
        ValuesOf,
        Maybe,
        SeiMessage
    }
}
export default IVSBroadcastClient;

/**
 * Jitter buffer configuration
 */
export declare interface JitterBufferConfiguration {
    /**
     * The desired minimum delay to apply to participants being subscribed to.
     * Note that this will not be applied if the local participant is publishing.
     *
     * The value can either be a preset or a custom number:
     *
     * Default: {@link JitterBufferMinDelay.DEFAULT}
     * Custom: `number`, value in millisceonds
     */
    minDelay?: JitterBufferMinDelay | number;
}

/****************************
 * Subscribe Configuration  *
 ****************************/
/**
 * Jitter buffer minimum delay presets
 */
export declare enum JitterBufferMinDelay {
    DEFAULT = "default",
    LOW = "low",
    MEDIUM = "medium",
    HIGH = "high"
}

/**
 * WebRTC stats from a local audio stream
 *
 * @public
 */
export declare interface LocalAudioStats {
    /**
     * The network quality associated with publishing local audio
     */
    networkQuality?: NetworkQuality;
    /**
     * The NACK count received by SDK for sending audio
     */
    nackCount?: number;
    /**
     * The number of audio packets sent
     */
    packetsSent?: number;
    /**
     * The number of audio packets retransmitted
     */
    retransmittedPacketsSent?: number;
    /**
     * The total number of audio bytes sent, including retransmissions
     */
    bytesSent?: number;
    /**
     * The number of RTP header bytes sent
     */
    headerBytesSent?: number;
    /**
     * The number of audio RTP packets lost
     */
    packetsLost?: number;
    /**
     * The number of retransmitted bytes
     */
    retransmittedBytesSent?: number;
    /**
     * The total time (in seconds) that packets have been buffered locally
     */
    totalPacketSendDelay?: number;
}

/**
 * A LocalStageStream is a wrapper for media coming from
 * local participants. It gives you the ability to perform
 * various actions like mute and unmute participants.
 */
export declare class LocalStageStream extends StageStream<LocalStageStreamEventMap> {
    mediaConfig: StageStreamConfiguration;
    private validator;
    private validatedExternalConfig?;
    private normalizedConfig;
    /**
     * Creates an instance of a LocalStageStream
     *
     * @param track - The MediaStreamTrack that this LocalStageStream wraps
     * @param config - Optional video or audio configuration on the stage
     */
    constructor(track: MediaStreamTrack, config?: StageVideoConfiguration | StageAudioConfiguration);
    /**
     * Updates the mute state of this LocalStageStream
     *
     * @param mute - The desired mute state of this LocalStageStream
     */
    setMuted: (mute: boolean) => void;
    /**
     * Inserts an SEI payload into the published stream. The stream must be
     * currently published by a stage's local participant using the H.264 codec,
     * and the inBandMessaging configuration must enabled via the
     * LocalStageStream's {@link StageVideoConfiguration}.
     *
     * The payload size must be greater than 0KB and less than 1KB, and the
     * number of SEI messages inserted must not exceed 10KB per
     * second.  Messages are not guaranteed to arrive, especially in bad network
     * conditions.
     *
     * @param {ArrayBuffer} payload - The ArrayBuffer payload.
     * @param {Object} [options] - Options for the insert operation.
     * @param {number} [options.repeatCount] - The number of times to repeat the
     * SEI message. Must be between 0 and 30.
     * @returns {Promise<void>} A Promise that resolves to undefined.
     */
    insertSeiMessage: (payload: ArrayBuffer, options?: {
        repeatCount?: number;
    }) => Promise<void>;
    /**
     * Request quality statistics for the local stream.
     *
     * This method provides access to detailed WebRTC statistics for the local stream,
     * including network quality, packets sent and lost, bitrate, and frame-related metrics.
     *
     * Note that in Firefox and Safari the API will report the `networkQuality`
     * as either `NORMAL` or `OFFLINE`. Safari's offline event is slower to fire.
     *
     * @returns A promise that resolves to an array of LocalAudioStats or LocalVideoStats depending on the stream type,
     * or undefined if statistics are not available
     * @public
     */
    requestQualityStats: () => Promise<LocalAudioStats[] | LocalVideoStats[] | undefined>;
}

declare type LocalStageStreamEventMap = {
    [LocalStageStreamEvents.LOCAL_STREAM_MUTE_CHANGED]: (stream: LocalStageStream) => void;
    [LocalStageStreamEvents.LOCAL_STREAM_INSERT_SEI_MSG_REQUEST]: (stream: LocalStageStream, payload: ArrayBuffer, options?: {
        repeatCount?: number;
    }) => void;
};

export declare enum LocalStageStreamEvents {
    LOCAL_STREAM_MUTE_CHANGED = "localStreamMutedChanged",
    LOCAL_STREAM_INSERT_SEI_MSG_REQUEST = "localStreamInsertSeiMsgRequest"
}

/**
 * WebRTC stats from a local video stream
 *
 * @public
 */
export declare interface LocalVideoStats {
    /**
     * The network quality associated with publishing local video
     */
    networkQuality?: NetworkQuality;
    /**
     * The NACK count received by SDK for sending video
     */
    nackCount?: number;
    /**
     * The number of video RTP packets sent
     */
    packetsSent?: number;
    /**
     * The number of video RTP packets lost
     */
    packetsLost?: number;
    /**
     * The number of video RTP packets retransmitted
     */
    retransmittedPacketsSent?: number;
    /**
     * The total number of video bytes sent, including retransmissions
     */
    bytesSent?: number;
    /**
     * The number of RTP header bytes sent
     */
    headerBytesSent?: number;
    /**
     * The number of retransmitted bytes
     */
    retransmittedBytesSent?: number;
    /**
     * The total number of seconds that RTP packets have been buffered locally
     */
    totalPacketSendDelay?: number;
    /**
     * The total number of Full Intra Request (FIR) packets received
     */
    firCount?: number;
    /**
     * The total number of Picture Loss Indication (PLI) packets received
     */
    pliCount?: number;
    /**
     * The number of video frames encoded
     */
    framesEncoded?: number;
    /**
     * The number of video key frames encoded
     */
    keyFramesEncoded?: number;
    /**
     * The total time spent encoding video frames (in seconds)
     */
    totalEncodeTime?: number;
    /**
     * Incremented by the target frame size in bytes every time a frame has been encoded
     */
    totalEncodedBytesTarget?: number;
    /**
     * Represents the total number of frames sent
     */
    framesSent?: number;
    /**
     * Represents the total number of huge frames sent
     */
    hugeFramesSent?: number;
    /**
     * The current reason for limiting the resolution and/or framerate
     */
    qualityLimitationReason?: string;
    /**
     * The total time spent in each quality limitation state (in seconds)
     */
    qualityLimitationResolutionChanges?: number;
    /**
     * Total time spent in quality limitation state due to CPU (in seconds)
     */
    qualityLimitationCpuDuration?: number;
    /**
     * Total time spent in quality limitation state due to restricted bandwidth (in seconds)
     */
    qualityLimitationBandwidthDuration?: number;
    /**
     * Indicates if SDK is actively sending local video or not
     */
    active?: boolean;
    /**
     * Represents the width of the last encoded frame
     */
    frameWidth?: number;
    /**
     * Represents the height of the last encoded frame
     */
    frameHeight?: number;
    /**
     * The number of encoded frames during the last second
     */
    framesPerSecond?: number;
    /**
     * Represents the layer used by simulcast; not empty only when simulcast is enabled
     */
    rid?: string;
    /**
     * A positive 32-bit integer uniquely identifying the SSRC of the RTP packets whose statistics are covered by this stream
     */
    ssrc?: number;
}

declare const LOG_LEVEL_TYPE_ERROR: {
    readonly name: "LogLevelTypeError";
    readonly code: 14000;
    readonly message: "Log Level must be a valid integer between [0..5]";
};

export declare type LogFn = (args: LogParams) => void;

export declare interface Logger {
    debug: LogFn;
    error: ErrorFn;
    info: LogFn;
    warn: LogFn;
}

declare const LOGGER_TYPE_ERROR: {
    readonly name: "LoggerTypeError";
    readonly code: 15000;
    readonly message: "Logger must be an object";
};

export declare interface LoggerParams {
    scope: LoggerScope;
}

declare enum LoggerScope {
    TIME = "Time",
    WEBRTC = "WebRTC",
    EMITTER = "Emitter",
    MEDIA = "Media",
    SUBSCRIPTION = "StageSubscription",
    PUBLICATION = "StagePublication",
    CONNECTION = "StageConnection",
    SOCKET = "StageSocket",
    BROADCAST_STAGE_CLIENT = "BroadcastStageClient",
    STAGE = "Stage",
    REMOTE_PARTICIPANT = "RemoteParticipant",
    LOCAL_PARTICIPANT = "LocalParticipant",
    CONFIGURATION = "Configuration",
    REMOTE_PLAYBACK = "RemotePlaybackController",
    REMOTE_STAGE_STREAM = "RemoteStageStream"
}

declare enum LogLevels {
    TRACE = 0,
    DEBUG = 1,
    INFO = 2,
    WARN = 3,
    ERROR = 4,
    SILENT = 5
}
export { LogLevels as LOG_LEVEL }
export { LogLevels }

export declare interface LogParams {
    [key: string]: unknown;
    label?: string;
    msg?: string;
    err?: Error | unknown;
    scope?: LoggerScope;
    metadata?: {
        [key: string]: unknown;
    };
    t?: number;
    level?: string;
}

export declare type Maybe<T> = T | undefined;

declare class MediaStreamManager {
    private mediaTracks;
    setTrack(type: StreamType, track: MediaStreamTrack): void;
    getTrack(type: StreamType): Maybe<MediaStreamTrack>;
}

declare const NETWORK_RECONNECT_CONFIGURATION_ERROR: {
    readonly name: "NetworkReconnectConfigurationError";
    readonly code: 17000;
    readonly message: "Error when configuring network reconnect";
};

declare const NETWORK_RECONNECT_CONFIGURATION_INVALID_TIMEOUT_ERROR: {
    readonly name: "NetworkReconnectConfigurationInvalidTimeoutError";
    readonly code: 17001;
    readonly message: `Network reconnect timeout value must be a number between [10000, ${number}]`;
};

/**
 * Network quality enum used to determine the network quality of local or remote
 * stage streams based on RTC statistics, i.e. analyzing the ratio of lost
 * packets to total packets sent/received.
 *
 * @public
 */
export declare const enum NetworkQuality {
    /** No packet loss */
    EXCELLENT = 4,
    /** Minimal packet loss, expect minimal to no media degradation */
    GOOD = 3,
    /** Low packet loss, expect media degradation at times */
    NORMAL = 2,
    /** Moderate packet loss, expect media degradation */
    POOR = 1,
    /** High packet loss or connection issues, expect extreme degradation, i.e. no video or audio */
    DOWN = 0
}

export declare interface NetworkReconnectConfig {
    reconnect: boolean;
    timeout?: number;
}

declare const PEER_CONNECTION_ERROR: {
    readonly name: "PeerConnectionError";
    readonly code: 10001;
    readonly message: "Peer connection has failed.";
};

declare const PEER_SETUP_ERROR: {
    readonly name: "PeerSetupError";
    readonly code: 10000;
    readonly message: "Unexpected return value from setup request";
};

declare type PeerClientStats = {
    action: Action;
    rawReport: RTCStatsReport;
} & ({
    action: Action.PUBLISH;
    stats: LocalAudioStats[] | LocalVideoStats[] | undefined;
} | {
    action: Action.SUBSCRIBE;
    stats: RemoteAudioStats[] | RemoteVideoStats[] | undefined;
});

/**
 * @internal
 */
export declare type PublishResponse = Promise<{
    peerConnection: RTCPeerConnection;
}>;

/**
 * WebRTC stats from a remote audio stream
 *
 * @public
 */
export declare interface RemoteAudioStats {
    /**
     * The network quality associated with receiving remote audio
     */
    networkQuality?: NetworkQuality;
    /**
     * The NACK count sent for receiving audio
     */
    nackCount?: number;
    /**
     * The number of audio packets received
     */
    packetsReceived?: number;
    /**
     * The number of audio RTP packets lost
     */
    packetsLost?: number;
    /**
     * The number of audio RTP packets discarded
     */
    packetsDiscarded?: number;
    /**
     * The total number of audio bytes received
     */
    bytesReceived?: number;
    /**
     * The number of RTP header bytes received
     */
    headerBytesReceived?: number;
    /**
     * Represents the sum of time in seconds that packets buffered in jitter buffer
     */
    jitterBufferDelay?: number;
    /**
     * The total number of samples that have come out of the jitter buffer
     */
    jitterBufferEmittedCount?: number;
    /**
     * The jitter measured in seconds for this audio source
     */
    jitter?: number;
    /**
     * The total number of audio samples received
     */
    totalSamplesReceived?: number;
    /**
     * The total number of audio samples that have been concealed
     */
    concealedSamples?: number;
    /**
     * The total number of samples that are silent concealed samples
     */
    silentConcealedSamples?: number;
    /**
     * The total number of concealment events
     */
    concealmentEvents?: number;
    /**
     * This is the counter of audio samples inserted to slow down playout
     */
    insertedSamplesForDeceleration?: number;
    /**
     * This is the counter of audio samples removed to speed up playout
     */
    removedSamplesForAcceleration?: number;
    /**
     * Represents the audio level of receiving audio, and the value is between 0 and 1
     */
    audioLevel?: number;
}

/**
 * A RemoteStageStream is a wrapper for media coming from remote participants. It's obtained
 * from the {@link StageEvents.STAGE_PARTICIPANT_STREAMS_ADDED} event and contains APIs for
 * manipulating the quality and state of the media playback.
 *
 * The `RemoteStageStream` emits {@link RemoteStageStreamEvents} which can be listened to
 * by calling `stream.on(event, callback)` or `stream.off(event, callback)`. The callback
 * types are defined in the {@link RemoteStageStreamEventMap}.
 */
export declare class RemoteStageStream extends TypedEmitter<RemoteStageStreamEventMap> implements StageStream<RemoteStageStreamEventMap> {
    private internalStream?;
    private cachedId;
    private cachedParticipantInfo;
    private cachedStreamType;
    private cachedMediaStreamTrack;
    private cachedIsMuted;
    private cachedIsAdapting;
    private cachedLayers;
    private cachedHighestQualityLayer?;
    private cachedLowestLowestLayer?;
    private cachedSelectedLayer?;
    /**
     * Creates an instance of a StageStream
     *
     * @param data - The track, info, and other properties required to create the stream
     * @internal
     */
    private constructor();
    private onAdaptionChanged;
    private onLayersChanged;
    private onLayerSelected;
    private updateCachedStreamData;
    /**
     * Internal method which is copied here for implementation completeness, but will
     * still no-op if invoked.
     *
     * @internal
     */
    setGetStats: () => void;
    /**
     * Returns the MediaTrack Id for the track associated
     * with the stream.
     */
    get id(): string;
    /**
     * Returns the {@link StageParticipantInfo} associated with this
     * stream.
     */
    get participantInfo(): StageParticipantInfo;
    /**
     * Returns the stream type associated with the stream.
     */
    get streamType(): StreamType;
    /**
     * Returns the MediaStreamTrack associated with the stream.
     */
    get mediaStreamTrack(): MediaStreamTrack;
    /**
     * Returns whether the audio or video stream is muted or not. A muted
     * audio stream is one that does not send audio samples (silent), and
     * a muted video stream is one that does not send video frames (frozen).
     */
    get isMuted(): boolean;
    /**
     * Mute or unmute a participant remotely. Note at this time this method is a no-op
     * as muting is controlled primarily by changing the {@link SubscribeType} on the
     * {@link StageStrategy}.
     *
     * @param muted - Boolean indicating whether to mute or unmute stream
     * @internal
     */
    set isMuted(muted: boolean);
    /**
     * Returns whether or not the server is dynamically adapting layers based on the
     * network conditions of the device (`true`), or a manual layer has been chosen
     * by the application (`false`).
     *
     * Use this value in conjunction with the {@link RemoteStageStream.getSelectedLayer} to determine whether the
     * selected layer is associated with Dynamic Simulcast or have been manually selected.
     *
     * For example:
     * - isAdapting === true --> getSelectedLayer is server selected layer
     * - isAdapting === false --> getSelectedLayer is app selected layer
     */
    get isAdapting(): boolean;
    /**
     * Request a snapshot of the RTCStatsReport from the stream.
     */
    requestRTCStats: () => Promise<RTCStatsReport | undefined>;
    /**
     * Request quality statistics for the remote stream.
     *
     * This method provides access to detailed WebRTC statistics for the remote stream,
     * including network quality, packet received and lost, jitter, and frame-related metrics.
     *
     * @returns A promise that resolves to an array of RemoteAudioStats or RemoteVideoStats depending on the stream type,
     * or undefined if statistics are not available
     * @public
     */
    requestQualityStats: () => Promise<RemoteAudioStats[] | RemoteVideoStats[] | undefined>;
    /**
     * This API is the previous internal API and now just defers internally to the requestRTCStats.
     *
     * @deprecated
     */
    getStats: () => Promise<RTCStatsReport | undefined>;
    /**
     * Mute or unmute a participant remotely. Note at this time this method is a no-op
     * as muting is controlled primarily by changing the {@link SubscribeType} on the
     * {@link StageStrategy}.
     *
     * @param mute - Boolean indicating whether to mute or unmute stream
     * @internal
     */
    setMuted: (mute: boolean) => void;
    /**
     * Returns the list of all current {@link StageStreamLayer}s. Note that this list will change
     * dynamically over time according to the publishers stream. For example, if the publisher only
     * is able to send two layers (e.g. low, mid) and stops sending hi, then this method will only
     * list the available layers for selection.
     */
    getLayers: () => StageStreamLayer[];
    /**
     * Return the currently selected layer. If the publisher is not sending multiple simulcast
     * layers, then no layer will be returned.
     *
     * If the publisher stops sending the selected layer, the server will attempt to select
     * the next best layer, which will be notified via the {@link RemoteStageStreamEvents.LAYER_SELECTED}.
     *
     * Use this value in conjunction with the {@link RemoteStageStream.isAdapting} flag to determine whether the
     * selected layer is associated with Dynamic Simulcast or have been manually selected.
     *
     * For example:
     * - isAdapting === true --> getSelectedLayer is server selected layer
     * - isAdapting === false --> getSelectedLayer is app selected layer
     */
    getSelectedLayer: () => StageStreamLayer | undefined;
    /**
     * Returns the lowest quality layer available. Quality here is defined first by dimension, then
     * bitrate, and finally framerate.
     *
     * For example, a 180p stream is lower quality than a 360p stream, even if (in the rare case)
     * the 180p stream has a higher bitrate, or framerate.
     */
    getLowestQualityLayer: () => StageStreamLayer | undefined;
    /**
     * Returns the highest quality layer available. Quality here is defined first by dimension, then
     * bitrate, and finally framerate.
     *
     * For example, a 1080p stream is higher quality than a 720p stream, even if (in the rare case)
     * the 720p stream has a higher bitrate, or framerate.
     */
    getHighestQualityLayer: () => StageStreamLayer | undefined;
    /**
     * Cleans up the Stage Stream so no additional events will fire. This event is fired for customers
     * so it does not need to be handled explicitly.
     *
     * @internal
     */
    cleanup(): void;
}

/**
 * A map of possible callbacks based on the provided {@link RemoteStageStreamEvents} key.
 * This map is not directly used and will be inferred when calling `stream.on` or `stream.off`.
 */
export declare type RemoteStageStreamEventMap = {
    [RemoteStageStreamEvents.LAYERS_CHANGED]: (layers: StageStreamLayer[]) => void;
    [RemoteStageStreamEvents.LAYER_SELECTED]: (layer: StageStreamLayer | undefined, reason: StageStreamLayerSelectedReason) => void;
    [RemoteStageStreamEvents.ADAPTION_CHANGED]: (adaption: boolean) => void;
};

/**
 * A enumeration of {@link RemoteStageStream} events which can be provided
 * to the `stream.on` or `stream.off` methods.
 */
export declare enum RemoteStageStreamEvents {
    /**
     * The `LAYERS_CHANGED` event is emitted any time the list of available
     * {@link StageStreamLayer}s changes. This could be the number of layers,
     * or individual properties like whether a new layer has been selected.
     *
     * This event can fire every couple of seconds if many selections are being
     * made either from the application, or the publishers network is unreliable
     * causing some layers to become inactive.
     *
     * @param payload - {@link StageStreamLayer}[]
     */
    LAYERS_CHANGED = "LAYERS_CHANGED",
    /**
     * The `LAYER_SELECTED` event is emitted when either the server selects
     * a layer when in dynamic adaption mode, or when the application selects
     * one via the {@link StageStrategy.preferredLayerForStream}.
     *
     * To determine the source of the selection, refer to the `ADAPTION_CHANGED`
     * event, or the {@link RemoteStageStream.isAdapting} getter.
     *
     * @param payload - {@link StageStreamLayer}, {@link StageStreamLayerSelectedReason}
     */
    LAYER_SELECTED = "LAYER_SELECTED",
    /**
     * The `ADAPTION_CHANGED` event is emitted whenever the server changes from
     * dynamic to manual adaption. Dynamic adaption means that the server will
     * choose the optimal layer for the device based on network conditions, whereas
     * manual adaption is what the application chooses via {@link StageStrategy.preferredLayerForStream}.
     *
     * @param payload - `boolean`
     */
    ADAPTION_CHANGED = "ADAPTION_CHANGED"
}

/**
 * WebRTC stats from a remote video stream
 *
 * @public
 */
export declare interface RemoteVideoStats {
    /**
     * The network quality associated with receiving remote video
     */
    networkQuality?: NetworkQuality;
    /**
     * The NACK count sent for receiving video
     */
    nackCount?: number;
    /**
     * The number of video RTP packets received
     */
    packetsReceived?: number;
    /**
     * The number of video RTP packets lost
     */
    packetsLost?: number;
    /**
     * The number of video RTP packets discarded
     */
    packetsDiscarded?: number;
    /**
     * The total number of video bytes received
     */
    bytesReceived?: number;
    /**
     * The number of RTP header bytes received
     */
    headerBytesReceived?: number;
    /**
     * Represents the sum of time in seconds that packets buffered in jitter buffer
     */
    jitterBufferDelay?: number;
    /**
     * The total number of samples that have come out of the jitter buffer
     */
    jitterBufferEmittedCount?: number;
    /**
     * The jitter measured in seconds for this video source
     */
    jitter?: number;
    /**
     * The total number of Picture Loss Indication (PLI) packets sent
     */
    pliCount?: number;
    /**
     * The total number of Full Intra Request (FIR) packets sent
     */
    firCount?: number;
    /**
     * The number of video frames received
     */
    framesReceived?: number;
    /**
     * The number of video frames decoded
     */
    framesDecoded?: number;
    /**
     * The number of key frames decoded
     */
    keyFramesDecoded?: number;
    /**
     * The total number of video frames dropped prior to decode
     */
    framesDropped?: number;
    /**
     * The total time spent decoding video frames (in seconds)
     */
    totalDecodeTime?: number;
    /**
     * The total inter-frame delay (in seconds)
     */
    totalInterFrameDelay?: number;
    /**
     * The total squared inter-frame delay (in seconds squared)
     */
    totalSquaredInterFrameDelay?: number;
    /**
     * The total number of video pauses experienced by receiver
     */
    pauseCount?: number;
    /**
     * The total duration of pauses in seconds
     */
    totalPausesDuration?: number;
    /**
     * The total number of video freezes experienced by receiver
     */
    freezeCount?: number;
    /**
     * The total duration of rendered frames which are considered frozen
     */
    totalFreezesDuration?: number;
    /**
     * Represents the width of the last decoded frame
     */
    frameWidth?: number;
    /**
     * Represents the height of the last decoded frame
     */
    frameHeight?: number;
    /**
     * The number of decoded frames during the last second
     */
    framesPerSecond?: number;
    /**
     * A positive 32-bit integer uniquely identifying the SSRC of the RTP packets whose statistics are covered by this stream
     */
    ssrc?: number;
}

declare const REMOVE_DEVICE_NOT_FOUND_ERROR: {
    readonly name: "RemoveDeviceNotFoundError";
    readonly code: 5000;
    readonly message: "Device not found";
};

declare const REMOVE_IMAGE_NOT_FOUND_ERROR: {
    readonly name: "RemoveImageNotFoundError";
    readonly code: 6000;
    readonly message: "Image not found";
};

declare class RequestUUID {
    value: string;
    constructor();
}

/**
 * The stream's configured resolution as defined by its width and height.
 * Must be a value between 160 and 1920, with 1920x1080 or 1080x1920 being maximum resolutions.
 */
export declare interface ResolutionConfig {
    /**
     * Integer representing the stream resolution's width in pixels.
     */
    width: number;
    /**
     * Integer representing the stream resolution's height in pixels.
     */
    height: number;
}

/**
 * Represents a User Data Unregistered Supplemental Enhancement Information
 * (SEI) uuid and payload parsed from the video bitstream.
 *
 * @property uuid The 16 byte uuid (uuid_iso_iec_11578)
 * @property payload The payload bytes
 * @public
 */
export declare type SeiMessage = {
    uuid: ArrayBuffer;
    payload: ArrayBuffer;
};

/** @internal */
export declare type SfuResource = {
    node: string;
    cluster: string;
};

/**
 * Error when duplicate layers are configured.
 *
 * @public
 */
export declare const SIMULCAST_DUPLICATE_LAYERS_CONFIGURED: ErrorParams;

/**
 * Error when the layer configuration conflicts with the root stream configuration.
 *
 * @public
 */
export declare const SIMULCAST_LAYER_CONFIGURATION_CONFLICT: ErrorParams;

/**
 * Error when the layer input framerate is insufficient.
 *
 * @public
 */
export declare const SIMULCAST_LAYER_INPUT_FRAMERATE_INSUFFICIENT: ErrorParams;

/**
 * Error when the layer input resolution is insufficient.
 *
 * @public
 */
export declare const SIMULCAST_LAYER_INPUT_RESOLUTION_INSUFFICIENT: ErrorParams;

/**
 * Error which is emitted when simulcast is enabled but the configuration is invalid.
 *
 * @public
 */
export declare const SIMULCAST_LAYER_INVALID_TYPE: ErrorParams;

/**
 * Error when the layer orientation does not match the input video track.
 *
 * @public
 */
export declare const SIMULCAST_LAYER_ORIENTATION_MISMATCH: ErrorParams;

/**
 * Error when the max bitrate is exceeded.
 *
 * @public
 */
export declare const SIMULCAST_MAX_LAYER_BITRATE_EXCEEDED: ErrorParams;

/**
 * Error when the max layer dimensions are exceeded.
 *
 * @public
 */
export declare const SIMULCAST_MAX_LAYER_DIMENSIONS_EXCEEDED: ErrorParams;

/**
 * Error when the max framerate is exceeded.
 *
 * @public
 */
export declare const SIMULCAST_MAX_LAYER_FRAMERATE_EXCEEDED: ErrorParams;

/**
 * Error emitted when more than the maximum number of layers are provided.
 *
 * @public
 */
export declare const SIMULCAST_MAX_LAYERS_EXCEEDED: ErrorParams;

/**
 * Error when the min bitrate is exceeded.
 *
 * @public
 */
export declare const SIMULCAST_MIN_LAYER_BITRATE_EXCEEDED: ErrorParams;

/**
 * Error when the min layer dimensions are exceeded.
 *
 * @public
 */
export declare const SIMULCAST_MIN_LAYER_DIMENSIONS_EXCEEDED: ErrorParams;

/**
 * Error when the min framerate is exceeded.
 *
 * @public
 */
export declare const SIMULCAST_MIN_LAYER_FRAMERATE_EXCEEDED: ErrorParams;

/**
 * Warning which is emitted when simulcast is enabled but the browser or device
 * does not support it. Examples are Firefox, Screen Shares, or stages with a
 * maximum bitrate below 700Kbps.
 *
 * @public
 */
export declare const SIMULCAST_UNSUPPORTED: ErrorParams;

/**
 * Configuration for Simulcast. Simulcast is a feature which allows publishers to encode
 * and send multiple different qualities of video. If no layers are specified then
 * a default 3 layers will be sent.
 *
 * The number of layers and what layers to send can be configured with the 'layers' property.
 *
 * @public
 */
export declare interface SimulcastConfiguration {
    /**
     * Whether to enable or disable Simulcast as a publisher
     *
     * @default false
     */
    enabled: boolean;
    /**
     * The array of Simulcast Layers to be sent by the Publisher. At most 3 layers can be provided,
     * and at minimum 1. Providing 0 layers, or an empty array will result in the default layers
     * being used internally. For audio-only defer to the strategy methods.
     *
     * See the {@link SimulcastLocalLayerConfiguration} object for information about layer restrictions
     * as well as potential errors.
     */
    layers?: SimulcastLocalLayerConfiguration[];
}

/**
 * Preset helper class which generates a collection of sane defaults to use
 * when configuring Simulcast local layers.
 *
 * @public
 */
export declare const SimulcastLayerPresets: Readonly<{
    /**
     * Default 720p Simulcast layer.
     */
    DEFAULT_720: Readonly<SimulcastLocalLayerConfiguration>;
    /**
     * Default 540p Simulcast Layer
     */
    DEFAULT_540: Readonly<SimulcastLocalLayerConfiguration>;
    /**
     * Default 480p Simulcast Layer
     */
    DEFAULT_480: Readonly<SimulcastLocalLayerConfiguration>;
    /**
     * Default 360p Simulcast Layer
     */
    DEFAULT_360: Readonly<SimulcastLocalLayerConfiguration>;
    /**
     * Default 270p Simulcast Layer
     */
    DEFAULT_270: Readonly<SimulcastLocalLayerConfiguration>;
    /**
     * Default 180p Simulcast Layer
     */
    DEFAULT_180: Readonly<SimulcastLocalLayerConfiguration>;
    /**
     * Default 160p Simulcast Layer
     */
    DEFAULT_160: Readonly<SimulcastLocalLayerConfiguration>;
}>;

/**
 * Configuration for a local Simulcast layer. This represents a single video encoding layer
 * to be published and sent to subscribers. All properties must be defined.
 *
 * @public
 */
export declare interface SimulcastLocalLayerConfiguration {
    /**
     * Maximum layer bitrate in Kbps. Note that if any of the layer bitrates
     * exceed that of the root maxBitrateKbps an error will be thrown indicating
     * layer bitrates exceed that of the overall stage bitrate maximums.
     *
     * - Max: {@link STAGE_MAX_BITRATE}
     * - Min: {@link STAGE_MIN_BITRATE}
     */
    maxBitrateKbps: number;
    /**
     * Maximum layer frames per second. Note that if any of the layer fps values
     * exceed that of the root {@link StageVideoConfiguration.maxFramerate}
     * an error will be thrown indicating layer fps exceed that of
     * the overall stage fps maximums.
     *
     * - Max: {@link STAGE_MAX_FRAMERATE}
     * - Min: {@link STAGE_MIN_FRAMERATE}
     */
    maxFramerate: number;
    /**
     * The target width of the layer. Note that the aspect ratio of the largest layer
     * will be used as a reference for all other layers. Aspect ratios cannot be mixed
     * across layers, and if they deviate by more than X then an error will be thrown.
     *
     * This is to guard against cases of mixing portrait and landscape video across layers.
     *
     * - Max: {@link STAGE_MAX_RESOLUTION}
     * - Min: {@link STAGE_MIN_RESOLUTION}
     */
    width: number;
    /**
     * The target height of the layer. Note that the aspect ratio of the largest layer
     * will be used as a reference for all other layers. Aspect ratios cannot be mixed
     * across layers, and if they deviate by more than X then an error will be thrown.
     *
     * This is to guard against cases of mixing portrait and landscape video across layers.
     *
     * - Max: {@link STAGE_MAX_RESOLUTION}
     * - Min: {@link STAGE_MIN_RESOLUTION}
     */
    height: number;
}

/**
 * Simulcast subscriber specific configuration which determines the
 * behavior of streams when a subscribe begins.
 *
 * Note that a resubscribe is typically required in order for these
 * preferences to take affect.
 *
 * @public
 */
export declare interface SimulcastSubscribeConfiguration {
    /**
     * What layer is initially delivered to the subscriber.
     */
    initialLayerPreference?: InitialLayerPreference;
}

/**
 * A class impementing the Stages SDK
 */
export declare class Stage extends TypedEmitter<StageEventMap> {
    private strategyWrapper;
    private token;
    private stageConnection;
    private connectionState;
    private participants;
    private localParticipant?;
    private connectionStateErrored;
    private logger;
    private analyticsTracker;
    private joinTraceId?;
    private configManager;
    private errorManager;
    /**
     * Creates an instance of a Stage
     *
     * @param token - The token to join this stage as
     * @param strategy - The {@link StageStrategy} to use for this Stage
     */
    constructor(token: string, strategy: StageStrategy);
    private setupListeners;
    private onConnectionStateChanged;
    /**
     * If a local participant has already been established in the case where SDK consumers
     * may call join multiple times in a session, re-use the local participant throughout
     * join attempts.
     */
    private setupLocalParticipantOnce;
    private createLocalParticipant;
    private onRemoteParticipantJoined;
    private setupRemoteParticipantListeners;
    private setupRemoteParticipantStreamEvents;
    private onRemoteParticipantLeft;
    private emitParticipantLeave;
    private onRemoteParticipantStateChanged;
    private onPublishStateChanged;
    private onParticipantSubscribeStateChanged;
    private onLocalStreamsAdded;
    private onLocalStreamsRemoved;
    private onStreamMuteChange;
    private onSeiMessageReceived;
    /**
     * Joins the stage
     */
    join(): Promise<void>;
    /**
     * Leaves the stage
     */
    leave: () => void;
    private onBeforeUnloadCleanup;
    private cleanup;
    /**
     * Re-executes the {@link StageStrategy}
     */
    refreshStrategy: () => void;
    /**
     * Updates and re-executes the {@link StageStrategy}
     *
     * @param strategy - The new {@link StageStrategy} for the stage
     */
    replaceStrategy(strategy: StageStrategy): void;
    private updateManagerWithExternalSubscribeConfig;
    /**
     * Get remote subscribe configuration with future context
     * @returns Result containing the remote subscribe configuration if successful
     */
    private getRemoteSubscribeConfiguration;
    private getInternalSubscribeOverrides;
    private updateStreamsToPublishIfNecessary;
    private syncTracksToPublish;
    private determineStreamUpdates;
    private updateStreamLayerForParticipantInfo;
    /**
     * Reads the StageDisconnectMessage code and handles the event.
     *
     * @returns void
     */
    private onReassignmentMessage;
    /**
     * Create and track an error event when reassignments are rejected
     *
     * @param reassignmentRejectionDetails - Details of the reassignment rejection
     */
    private trackReassignmentRequestRejected;
    /**
     * Create and track a reassignment request event.
     *
     * @param message - The reassignment request message
     */
    private trackReassignmentRequest;
    /**
     * Handle incompatible codec signal from Mercury. The publisher should ensure this
     * is not being used as the active codec.
     */
    private onIncompatibleCodecsMessage;
}

/**
 * Maximum audio bitrate a stream can be encoded at.
 *
 * @public
 */
export declare const STAGE_MAX_AUDIO_BITRATE_KBPS = 128;

/**
 * Default audio bitrate a stream is encoded at.
 *
 * @public
 */
export declare const STAGE_MAX_AUDIO_BITRATE_KBPS_DEFAULT = 64;

/**
 * Maximum video bitrate a stream can be encoded at
 *
 * @public
 */
export declare const STAGE_MAX_BITRATE = 2500;

/**
 * Maximum video framerate a stream can be encoded at
 *
 * @public
 */
export declare const STAGE_MAX_FRAMERATE = 30;

/**
 * Maximum video resolution dimension a stream can be encoded at
 *
 * @public
 */
export declare const STAGE_MAX_RESOLUTION = 1280;

/**
 * Minimum audio bitrate a stream can be encoded at
 *
 * @public
 */
export declare const STAGE_MIN_AUDIO_BITRATE_KBPS = 12;

/**
 * Minimum video bitrate a stream can be encoded at
 *
 * @public
 */
export declare const STAGE_MIN_BITRATE = 50;

/**
 * Minimum video frame rate a stream can be encoded at
 *
 * @public
 */
export declare const STAGE_MIN_FRAMERATE = 10;

/**
 * Minimum video resolution dimension a stream can be encoded at
 *
 * @public
 */
export declare const STAGE_MIN_RESOLUTION = 160;

declare const STAGE_TOKEN_TYPE_ERROR: {
    readonly name: "StageTokenTypeError";
    readonly code: 19000;
    readonly message: "Stage token must be a string";
};

declare const STAGE_WHIP_OVERRIDE_ERROR: {
    readonly name: "StageWhipUrlOverrideError";
    readonly code: 19001;
    readonly message: "Stage WHIP override URL must be a string starting with http";
};

export declare interface StageAudioConfiguration {
    /**
     * Max bitrate of the audio stream in Kbps. Value is limited as follows:
     *
     * - Min: {@link STAGE_MIN_AUDIO_BITRATE_KBPS}
     * - Max: {@link STAGE_MAX_AUDIO_BITRATE_KBPS}
     * - Default {@link STAGE_MAX_AUDIO_BITRATE_KBPS_DEFAULT}
     */
    maxAudioBitrateKbps?: number;
    /**
     * Explicitly enable stereo (true) or mono (false) for the audio stream.
     *
     * Note: that if using a audio source retrieved with getUserMedia you must
     * disable autoGainControl, echoCancellation, and noiseSuppression
     * constraints when enabling stereo.
     *
     * @default false
     */
    stereo?: boolean;
}

/**
 * Class describing Stage related errors encountered when using the AmazonIVSBroadcastStageClient.
 */
declare class StageClientError extends BroadcastClientError {
    action: Action;
    token: StageToken;
    traceId: TraceId;
    code: number;
    tag: string;
    location: string;
    message: string;
    details?: string;
    remoteParticipantId?: string;
    requestUUID?: RequestUUID;
    fatal: boolean;
    nominal: boolean;
    count: number;
    /**
     * Creates the new error.
     *
     * @param parameters - Object containing the Error parameters.
     * @param parameters.token - The stage token.
     * @param parameters.traceId - The traceID for the stage error.
     * @param parameters.code - The code for the error.
     * @param parameters.tag - The error tag.
     * @param parameters.location - Location in code where error occurs.
     * @param parameters.message - The error message.
     * @param parameters.details - The error details.
     * @param parameters.remoteParticipantId - The remote participant ID for this stage user, if applicable.
     * @param parameters.requestUUID - The request UUID, if applicable.
     * @param parameters.fatal - This error ends the broadcast or attempt.
     * @param parameters.nominal - This is an expected failure.
     */
    constructor({ token, traceId, code, tag, location, message, details, remoteParticipantId, requestUUID, fatal, nominal, action, count, }: StageErrorParams);
}

export declare enum StageConnectionState {
    /**
     * SDK is not connected to Stage session
     */
    DISCONNECTED = "disconnected",
    /**
     * SDK is establishing connection to Stage session
     */
    CONNECTING = "connecting",
    /**
     * SDK is connected to Stage session
     */
    CONNECTED = "connected",
    /**
     * SDK errored in the middle of a session
     */
    ERRORED = "errored"
}

/**
 * Provides information about an error encountered by the SDK during an
 * operation.
 *
 * @interface StageError
 * @public
 */
export declare interface StageError {
    /**
     * Name of the error
     */
    readonly name: string;
    /**
     * {@link StageErrorCode} of the error
     */
    readonly code: StageErrorCode;
    /**
     * {@link StageErrorCategory} of the error
     */
    readonly category: StageErrorCategory;
    /**
     * Message of the error
     */
    readonly message: string;
}

/**
 * The category of an error. The category of the error is determined based on if
 * it is related to the connection to the stage (i.e. `JOIN_ERROR`), sending
 * media to the stage (i.e.`PUBLISH_ERROR`), or receiving an incoming media
 * stream from the stage (i.e. `SUBSCRIBE_ERROR`).
 *
 * @public
 */
export declare enum StageErrorCategory {
    /**
     * An error related to the connection to the stage.
     */
    JOIN_ERROR = "JOIN_ERROR",
    /**
     * An error related to publishing or sending media stream to the stage.
     */
    PUBLISH_ERROR = "PUBLISH_ERROR",
    /**
     * An error related to subscribing or receiving media stream from the stage.
     */
    SUBSCRIBE_ERROR = "SUBSCRIBE_ERROR"
}

/**
 * Stage error codes. Stage error codes are returned when a problem is
 * encountered that the SDK cannot recover from and require app intervention.
 *
 * @public
 */
export declare enum StageErrorCode {
    /**
     * Token is malformed. Ensure the token is not being unexpectantly modified
     * (i.e. spaces inserted, characters changed, etc.) before being passed to
     * the stage. Create a valid token and retry joining.
     */
    TOKEN_MALFORMED = 1,
    /**
     * Token is expired. Create a non-expired token and retry joining.
     */
    TOKEN_EXPIRED = 2,
    /**
     * Operation timed out. If the {@link StageErrorCategory} is `JOIN_ERROR`
     * then check if the stage exists and the token is valid. If the stage
     * exists and the token is valid, this failure is likely a network issue. In
     * that case, wait for the devices connectivity to recover and either
     * refresh the strategy or leave and rejoin the stage.
     */
    TIMEOUT = 3,
    /**
     * Operation failed. Fatal condition encountered attempting operation. Check
     * error details.
     */
    FAILED = 4,
    /**
     * Operation canceled. Check application code and ensure no repeated
     * joins/leaves/refreshStrategy/replaceStrategy invocations which may cause
     * repeated operations to be started and canceled before completion.
     */
    CANCELED = 5,
    /**
     * Stage is at capacity. Try again when stage is no longer at capacity.
     */
    STAGE_AT_CAPACITY = 6,
    /**
     * Codec is not supported by the stage. Check browser/platform for codec
     * support.
     */
    CODEC_MISMATCH = 7,
    /**
     * Token does not have permission for the operation. Recreate the token with
     * the correct permission(s) and try again.
     */
    TOKEN_NOT_ALLOWED = 8,
    /**
     * If a stage is deleted while a participant is actively in a session, they will be
     * gracefully disconnected. If a participant tries to join a deleted stage, an error
     * will be thrown during the join establishment.
     */
    STAGE_DELETED = 9,
    /**
     * If a participant is disconnected while actively in a session, they will be
     * gracefully disconnected. If a participant tries to join again, an error
     * will be thrown during the join establishment.
     */
    PARTICIPANT_DISCONNECTED = 10
}

declare interface StageErrorParams {
    action: Action;
    token: StageToken;
    traceId: TraceId;
    code: number;
    tag: string;
    location: string;
    message: string;
    details?: string;
    remoteParticipantId?: string;
    requestUUID?: RequestUUID;
    nominal: boolean;
    fatal: boolean;
    count?: number;
}

declare type StageEventMap = {
    [StageEvents.STAGE_CONNECTION_STATE_CHANGED]: (state: StageConnectionState) => void;
    [StageEvents.STAGE_PARTICIPANT_JOINED]: (participantInfo: StageParticipantInfo) => void;
    [StageEvents.STAGE_PARTICIPANT_LEFT]: (participantInfo: StageParticipantInfo) => void;
    [StageEvents.STAGE_PARTICIPANT_PUBLISH_STATE_CHANGED]: (participantInfo: StageParticipantInfo, state: StageParticipantPublishState) => void;
    [StageEvents.STAGE_PARTICIPANT_SUBSCRIBE_STATE_CHANGED]: (participantInfo: StageParticipantInfo, state: StageParticipantSubscribeState) => void;
    [StageEvents.STAGE_PARTICIPANT_STREAMS_ADDED]: (participantInfo: StageParticipantInfo, streams: StageStream[]) => void;
    [StageEvents.STAGE_PARTICIPANT_STREAMS_REMOVED]: (participantInfo: StageParticipantInfo, streams: StageStream[]) => void;
    [StageEvents.STAGE_STREAM_SEI_MESSAGE_RECEIVED]: (participantInfo: StageParticipantInfo, message: SeiMessage) => void;
    [StageEvents.STAGE_STREAM_MUTE_CHANGED]: (participantInfo: StageParticipantInfo, stream: StageStream) => void;
    [StageEvents.STAGE_STREAM_LAYERS_CHANGED]: (participantInfo: StageParticipantInfo, stream: RemoteStageStream, layers: StageStreamLayer[]) => void;
    [StageEvents.STAGE_STREAM_LAYER_SELECTED]: (participantInfo: StageParticipantInfo, stream: RemoteStageStream, layer: StageStreamLayer | undefined, reason: StageStreamLayerSelectedReason) => void;
    [StageEvents.STAGE_STREAM_ADAPTION_CHANGED]: (participantInfo: StageParticipantInfo, stream: RemoteStageStream, adaption: boolean) => void;
    [StageEvents.ERROR]: (error: StageError) => void;
    [StageEvents.STAGE_LEFT]: (reason: StageLeftReason) => void;
};

/**
 * Stage events that can be listened to with `stage.on` and `stage.off`.
 *
 * @public
 */
export declare enum StageEvents {
    /**
     * Invoked when the local connection to the stage has changed.
     *
     * @param payload - {@link StageConnectionState}
     */
    STAGE_CONNECTION_STATE_CHANGED = "stageConnectionStateChanged",
    /**
     * Indicates that a remote participant joined the stage.
     *
     * @param payload - {@link StageParticipantInfo}
     */
    STAGE_PARTICIPANT_JOINED = "stageParticipantJoined",
    /**
     * Indicates that a participant left the stage.
     *
     * @param payload - {@link StageParticipantInfo}
     */
    STAGE_PARTICIPANT_LEFT = "stageParticipantLeft",
    /**
     * Indicates that a stream(s) have been added for a participant.
     *
     * @param payload - {@link StageParticipantInfo}, {@link StageStream}[]
     */
    STAGE_PARTICIPANT_STREAMS_ADDED = "stageParticipantStreamsAdded",
    /**
     * Indicates that a stream(s) have been removed for a participant.
     *
     * @param payload - {@link StageParticipantInfo}, {@link StageStream}[]
     */
    STAGE_PARTICIPANT_STREAMS_REMOVED = "stageParticipantStreamsRemoved",
    /**
     * Indicates that a stream's mute state has changed
     *
     * @param payload - {@link StageParticipantInfo}, {@link StageStream}
     */
    STAGE_STREAM_MUTE_CHANGED = "stageStreamMuteChanged",
    /**
     * Indicates an SEI message was received
     *
     * @param message - {@link SeiMessage}
     */
    STAGE_STREAM_SEI_MESSAGE_RECEIVED = "stageStreamSeiMessageReceived",
    /**
     * Indicates that a stream's available layers state has been changed
     *
     * @param payload - {@link StageParticipantInfo}, {@link StageStream}, {@link StageStreamLayer}[]
     */
    STAGE_STREAM_LAYERS_CHANGED = "stageStreamLayersChanged",
    /**
     * Indicates that a stream layer has been selected based on the preferredLayerForStream strategy
     *
     * @param payload - {@link StageParticipantInfo}, {@link StageStream}, {@link StageStreamLayer}, {@link StageStreamLayerSelectedReason}
     */
    STAGE_STREAM_LAYER_SELECTED = "stageStreamLayerSelected",
    /**
     * Indicates that a stream's adaption state has changed, and whether it will automatically adapt
     * based on network conditions, or be controlled manually based on the preferredLayerForStream
     * values.
     *
     * @param payload - {@link StageParticipantInfo}, {@link StageStream}, `boolean`
     */
    STAGE_STREAM_ADAPTION_CHANGED = "stageStreamAdaptionChanged",
    /**
     * Indicates a change to participant subscribtion state
     *
     * @param payload - {@link StageParticipantInfo}, {@link StageParticipantSubscribeState}
     */
    STAGE_PARTICIPANT_SUBSCRIBE_STATE_CHANGED = "stageParticipantSubscribeStateChanged",
    /**
     * Indicates a change to participant publish state
     *
     * @param payload - {@link StageParticipantInfo}, {@link StageParticipantPublishState}
     */
    STAGE_PARTICIPANT_PUBLISH_STATE_CHANGED = "stageParticipantPublishStateChanged",
    /**
     * Indicates the stage has encountered an error.
     *
     * @param payload - {@link StageError}
     * @public
     */
    ERROR = "error",
    /**
     * Indicates the stage has been left by the client
     *
     * @param payload - {@link StageLeftReason}
     * @public
     */
    STAGE_LEFT = "stageLeft"
}

/**
 * Reasons the stage was disconnected.
 *
 * @public
 * @enum
 */
export declare enum StageLeftReason {
    /**
     * Unknown reason, usually due to the server explicitly disconnecting the
     * connection.
     */
    UNKNOWN = "unknown",
    /**
     * Explicit invocation of leave() by the client.
     */
    USER_INITIATED = "user-initiated",
    /**
     * Token was reused by another stage instance and this instance was
     * disconnected. A token cannot be used to maintain more than one connection
     * to a stage at once.
     */
    TOKEN_REUSED = "token-reused",
    /**
     * This token was disconnected by the DisconnectParticipant API. This token
     * can no longer be used.
     */
    PARTICIPANT_DISCONNECTED = "participant-disconnected",
    /**
     * The stage the token was tied to was deleted. This token can no longer be
     * used.
     */
    STAGE_DELETED = "stage-deleted"
}

declare enum StageParticipantCapabilities {
    /**
     * A participant can publish.
     */
    PUBLISH = "publish",
    /**
     * A participant can be subscribed to or not.
     */
    SUBSCRIBE = "subscribe"
}

export declare type StageParticipantInfo = {
    readonly id: string;
    readonly userId: string;
    readonly attributes: Record<string, unknown>;
    readonly capabilities: Set<StageParticipantCapabilities>;
    readonly isLocal: boolean;
    readonly userInfo: Record<string, unknown>;
    videoStopped: boolean;
    audioMuted: boolean;
    isPublishing: boolean;
};

export declare enum StageParticipantPublishState {
    NOT_PUBLISHED = "not_published",
    ATTEMPTING_PUBLISH = "attempting_publish",
    PUBLISHED = "published",
    ERRORED = "errored"
}

export declare enum StageParticipantSubscribeState {
    NOT_SUBSCRIBED = "not_subscribed",
    ATTEMPTING_SUBSCRIBE = "attempting_subscribe",
    SUBSCRIBED = "subscribed",
    ERRORED = "errored"
}

/**
 * A StageStrategy dictates certain behavior on the {@link Stage}
 *
 * @public
 */
export declare interface StageStrategy {
    /**
     * Should return the local streams that should be published to the stage.
     * This will be invoked during the {@link Stage.join} call and every subsequent {@link Stage.refreshStrategy} call.
     * This will not be invoked if {@link StageStrategy.shouldPublishParticipant} returns false
     */
    stageStreamsToPublish(): LocalStageStream[];
    /**
     * Should return true if you wish to publish to the {@link Stage} Otherwise return false.
     */
    shouldPublishParticipant(participant: StageParticipantInfo): boolean;
    /**
     * Should return whether or not you wish to subscribe to a given participant. Return {@link SubscribeType.AUDIO_VIDEO}
     * to subscribe to audio and video. Return {@link SubscribeType.AUDIO_ONLY} to subscribe to just audio. Return {@link SubscribeType.NONE}
     * to not subscribe at all. This will be invoked after a new participant joins or against every participant
     * when {@link Stage.refreshStrategy} is called
     */
    shouldSubscribeToParticipant(participant: StageParticipantInfo): SubscribeType;
    /**
     * Optionally return the subscribe configuration for a given participant. If given, the returned {@link SubscribeConfiguration} will be merged
     * against SDK default values. If not given, the default values will be applied.
     */
    subscribeConfiguration?(participant: StageParticipantInfo): SubscribeConfiguration;
    /**
     * Optionally return the preferred stream layer for a given participant. If defined, the returned {@link StageStreamLayer} will be used
     * to change what video layer is delivered. This is used when a Publisher is sending Simulcast Layers, and allows the Subscriber
     * to choose between those layers.
     *
     * The strategy method can return:
     * - The layer object directly based on what {@link RemoteStageStream.getLayers} returns
     * - The layer object label string based on {@link StageStreamLayer.label}
     * - Undefined or null, which indicates that no layer should be selected, and dynamic adaption is preferred
     */
    preferredLayerForStream?(participant: StageParticipantInfo, stream: RemoteStageStream): StageStreamLayer | string | undefined | null;
}

/**
 * Base Class for common StageStream functionality
 */
export declare class StageStream<T extends EventMap = never> extends TypedEmitter<T> {
    id: string;
    streamType: StreamType;
    mediaStreamTrack: MediaStreamTrack;
    isMuted: boolean;
    /**
     * @internal
     */
    protected _getInternalStats?: GetStatsFunction;
    /**
     * Initializer for StageStream
     *
     * @param track - Media stream track to be wrapped
     * @param getStats - Optional callback override for WebRTC stats
     */
    constructor(track: MediaStreamTrack, getStats?: GetStatsFunction);
    /**
     * Used for setting the getStats call after publishing
     *
     * @param getStats - Callback for returning getStats callback
     * @internal
     */
    setGetStats: (getStats: GetStatsFunction) => void;
    /**
     * This API is the previous internal API and now just defers internally to the requestRTCStats.
     *
     * @deprecated
     */
    getStats: () => Promise<RTCStatsReport | undefined>;
    /**
     * Get the RTC Stats report for this media source
     */
    requestRTCStats: () => Promise<RTCStatsReport | undefined>;
    /**
     * Prevents the getStats closure from preventing garbage collection for
     */
    cleanup(): void;
}

/**
 * Consolidated stream configuration for audio or video tracks. Local Stream configuration
 * will be updated to include a fully normalized configuration filling the gaps of any configuration not
 * provided upon instantiation.
 *
 * @public
 */
export declare type StageStreamConfiguration = Required<StageVideoConfiguration> & Required<StageAudioConfiguration>;

/**
 * A layer is one quality or variation of media which is being sent from
 * the server from the publisher of the Stage.
 *
 * Layers can be retrieved a number of ways. At this time they are only
 * available for {@link RemoteStageStream}s with a type of {@link StreamType.VIDEO}:
 * - {@link RemoteStageStream.getLayers}
 * - {@link RemoteStageStream.getSelectedLayer}
 * - {@link RemoteStageStream.getHighestQualityLayer}
 * - {@link RemoteStageStream.getLowestQualityLayer}
 * - {@link RemoteStageStreamEvents.LAYER_SELECTED}
 * - {@link RemoteStageStreamEvents.LAYERS_CHANGED}
 * - {@link StageEvents.STAGE_STREAM_LAYERS_CHANGED}
 * - {@link StageEvents.STAGE_STREAM_LAYER_SELECTED}
 */
export declare type StageStreamLayer = {
    /**
     * The label is a generic string derived from the publisher. Typically this
     * is a readable name that describes the layer (e.g. `hi`, `mid`, `low`).
     *
     * This label is not guaranteed to be a specific value, and may change
     * dynamically so it should not be used as a key, or reliable value in
     * the application.
     */
    label: string;
    /**
     * The bitrate in bits per second of the layer. This value represents
     * the maximum possible bitrate of the publisher encoder. It may change
     * if the publishers network degrades or improves, either decreasing or
     * increasing the value.
     */
    bitrateBps: number;
    /**
     * The width of the layer in pixels. Note that this is only relevant for video
     * layers. This value represents the maximum possible width of the publishers
     * encoder. It may change if the publishers network degrades or improves, either decreasing or
     * increasing the value.
     */
    width: number;
    /**
     * The height of the layer in pixels. Note that this is only relevant for video
     * layers. This value represents the maximum possible height of the publishers
     * encoder. It may change if the publishers network degrades or improves, either decreasing or
     * increasing the value.
     *
     * Note that height is used to create the traditional quality mappings, and is
     * useful if you want to display quality selectors to users.
     *
     * e.g. height of `360 pixels` corresponds with `360p`
     */
    height: number;
    /**
     * The frame rate of the layer. Note that this is only relevant for video
     * layers. This value represents the maximum possible frame rate of the publishers
     * encoder. It may change if the publishers network degrades or improves, either decreasing or
     * increasing the value.
     */
    framesPerSecond: number;
    /**
     * A boolean describing whether the layer is currently selected by the server. A selected
     * layer is one that is available to display and is sending media. At this time only one
     * layer can be delivered at a time.
     */
    selected: boolean;
};

/**
 * When a layer is selected, it's possible that the layer is no longer available. This enumeration
 * helps to provide context to layer selections, and if they were unsuccessful due to unavailable
 * layers.
 */
export declare enum StageStreamLayerSelectedReason {
    /**
     * When a layer is unavailable, this means the SDK or server can no longer
     * surface the layer, and a new best effort layer has been selected.
     * Layer availability will also be notified via the onLayersChanged event
     */
    UNAVAILABLE = "unavailable",
    /**
     * The preferred layer set by the application or server has been selected
     * successfully.
     */
    SELECTED = "selected"
}

declare class StageToken {
    private analyticsTracker;
    rawToken: string;
    header: Record<string, unknown> | undefined;
    claims: Claims;
    signature: string;
    stageARN: string;
    participantID: string;
    eventsURL: string;
    whipURL: string;
    topic: string;
    versionFlags?: number;
    userID?: string;
    expirationTime?: number;
    attrGsRole?: string;
    attrGsSessionId?: string;
    customerId: string;
    constructor(encodedToken: string, analyticsTracker: AnalyticsTracker);
    getPublishEndpoint(): string;
    getSubscribeEndpoint(participantID: string): string;
    assertTokenIsUnexpired(traceId: TraceId, tag: string, location: string, identifier?: string): void;
    private parseVersionFlags;
    shouldSendSilentAudio(): boolean;
}

export declare interface StageVideoConfiguration {
    /**
     * Note: This property is currently unused and a no-op internally
     */
    minBitrate?: number;
    /**
     * Note: This property is deprecated in favor of maxVideoBitrateKbps
     *
     * @deprecated
     */
    maxBitrate?: number;
    /**
     * Maximum video bitrate in Kbps
     *
     * - Max: {@link STAGE_MAX_BITRATE}
     * - Min: {@link STAGE_MIN_BITRATE}
     * - Default: {@link STAGE_MAX_BITRATE}
     */
    maxVideoBitrateKbps?: number;
    /**
     * Frames per second
     *
     * - Max: {@link STAGE_MAX_FRAMERATE}
     * - Min: {@link STAGE_MIN_FRAMERATE}
     * - Default: {@link STAGE_MAX_FRAMERATE}
     */
    maxFramerate?: number;
    /**
     * Configuration for Simulcast (Layered Encoding)
     */
    simulcast?: SimulcastConfiguration;
    /**
     * Configuration for in-band messaging (i.e. inserting SEI messages)
     */
    inBandMessaging?: InBandMessagingStageVideoConfiguration;
}

/**
 * A static `landscape` stream configuration for a `Standard` IVS account.
 *
 * @category Stream Config
 */
export declare const STANDARD_LANDSCAPE: StreamConfig;

/**
 * A static `portrait` stream configuration for a `Standard` IVS account.
 *
 * @category Stream Config
 */
export declare const STANDARD_PORTRAIT: StreamConfig;

declare const STREAM_CONFIGURATION_ERROR: {
    readonly name: "StreamConfigurationError";
    readonly code: 12000;
    readonly message: "Error when configuring stream";
};

declare const STREAM_KEY_INVALID_CHAR_ERROR: {
    readonly name: "StreamKeyInvalidCharError";
    readonly code: 18000;
    readonly message: "Streamkey must contain only [A-Za-z0-9_-] characters";
};

/**
 * The configuration for a stream detailing its maximum capabilities.
 * maxFramerate must be between 10-60, and maxBitrate must be between 200-8500 kbps
 */
export declare interface StreamConfig {
    maxResolution: ResolutionConfig;
    maxFramerate: number;
    maxBitrate: number;
}

export declare enum StreamType {
    VIDEO = "video",
    AUDIO = "audio"
}

/**
 * Subscribe configuration, applies to participants who are being subscribed to.
 */
export declare interface SubscribeConfiguration {
    /**
     * Optional configuration to modify the jitter buffer
     */
    jitterBuffer?: JitterBufferConfiguration;
    /**
     * Optional configuration to enable receiving in-band messages (e.g. SEI)
     */
    inBandMessaging?: InBandMessagingSubscribeConfiguration;
    /**
     * Optional configuration to apply specific simulcast, like initial layers
     */
    simulcast?: SimulcastSubscribeConfiguration;
}

/**
 * @internal
 */
export declare type SubscribeRequest = {
    identifier: string;
    silentAudioTrack?: MediaStreamTrack;
    token?: string;
};

/**
 * @internal
 */
export declare type SubscribeResponse = Promise<{
    peerConnection: RTCPeerConnection;
}>;

export declare enum SubscribeType {
    /**
     * Do not subscribe to a participant.
     */
    NONE = "none",
    /**
     * Subscribe to participant's audio.
     */
    AUDIO_ONLY = "audio_only",
    /**
     * Subscribe to participant's audio and video.
     */
    AUDIO_VIDEO = "audio_video"
}

declare class TraceId {
    value: string;
    constructor();
}

/**
 * @internal
 */
export declare type TraceRequest = {
    token: string;
    message: string;
    remoteParticipantID?: string;
};

/**
 * A typed event emitter.
 *
 * To use simply declare your events then create your emitter with a generic type:
 * ```typescript
 * type MyEvents = {
 *   foo: (bar: string) => void;
 * }
 *
 * const emitter = new TypedEmitter<MyEvents>();
 *
 * emitter.emit('foo', 'bar') // <- This is valid
 *
 * emitter.emit('foo', false) // <- This is a type error
 * emitter.emit('bar', 'baz') // <- This is also a type error
 * ```
 *
 * @internal
 */
declare class TypedEmitter<Events extends EventMap> {
    private emitter;
    /**
     * Creates a new TypedEmitter
     */
    constructor();
    /**
     * Returns a boolean indicating whether an event has actively registered
     * listeners.
     * @param event - the event to register the callback for
     */
    hasListenersFor<Event extends keyof Events>(event: Event): boolean;
    /**
     * Register a new callback based on the **event** provided. The events enum
     * is either linked in the EventMap type or in the root of the class.
     * @param event - the event to register the callback for
     * @param callback - the callback to be invoked for the event
     */
    on<Event extends keyof Events>(event: Event, callback: Events[Event], context?: any): void;
    /**
     * Deregister a new callback based on the **event** provided. The events enum
     * is either linked in the EventMap type or in the root of the class.
     * @param event - the event to de-register the callback for
     * @param callback - the callback to be de-registered
     */
    off<Event extends keyof Events>(event: Event, callback: Events[Event], context?: any): void;
    /**
     * Emit args for event E
     * @param event - the event name
     * @param args - the args for the event
     * @internal
     */
    emit<E extends keyof Events>(event: E, ...args: Parameters<Events[E]>): void;
    /**
     * Remove all event listeners
     * @internal
     */
    removeAllListeners(): void;
}

declare const UPDATE_VIDEO_DEVICE_COMPOSITION_INDEX_MISSING_ERROR: {
    readonly name: "UpdateVideoDeviceCompositionIndexMissingError";
    readonly code: 7001;
    readonly message: "VideoComposition's \"index\" property is missing";
};

declare const UPDATE_VIDEO_DEVICE_COMPOSITION_MISSING_ERROR: {
    readonly name: "UpdateVideoDeviceCompositionMissingError";
    readonly code: 7000;
    readonly message: "VideoComposition is missing";
};

declare const UPDATE_VIDEO_DEVICE_COMPOSITION_NAME_NOT_FOUND_ERROR: {
    readonly name: "UpdateVideoDeviceCompositionNameMissingError";
    readonly code: 7002;
    readonly message: "Video device with that name is not found";
};

/** @internal */
export declare type ValuesOf<T> = T[keyof T];

/**
 * An interface to describe the position of a video device in the composite stream.
 */
export declare interface VideoComposition {
    /**
     * Z-index value. Devices with a higher index will be placed on top of devices with a lower index.
     */
    index: number;
    /**
     * Top-left x-coordinate of the device.
     */
    x?: number;
    /**
     * Top-left y-coordinate of the device.
     */
    y?: number;
    /**
     * Height of the device.
     */
    height?: number;
    /**
     * Width of the device.
     */
    width?: number;
}

/**
 * An interface to describe a video device.
 */
export declare interface VideoDevice {
    /**
     * Unique name given to this device.
     */
    name: string;
    /**
     * CanvasImageSource for this device.
     */
    element: CanvasImageSource;
    /**
     * {@link VideoComposition} for this device.
     */
    position: VideoComposition;
    /**
     * MediaStream or CanvasImageSource of this device.
     */
    source: MediaStream;
    /**
     * Boolean to determine whether or not to render this source
     */
    render: boolean;
}

/** @internal */
export declare type WHIPResource = {
    location?: string;
    sessionDeleteLink?: string;
    mediaControls?: string;
    layerControls?: string;
};

/** @internal */
export declare type WHIPServerResponse = {
    url: string;
    sdpAnswer: string;
    location?: string;
    sessionDeleteLink?: string;
    mediaControls?: string;
    layerControls?: string;
};

export { }
